\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{float}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\newcommand{\ProjectName}{ESF }

\begin{document}

\title{System Verification and Validation Plan for \ProjectName{}} 
\author{Alaap Grandhi}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
February 27 & 1.0 & Initial version of the VnV Plan\\
April 17 & 2.0 & Updated according to Professor Smith's feedback\\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables

\newpage

\pagenumbering{arabic}

This document will describe the Verification and Validation (VnV) plan for the \ProjectName{}project. It will
outline the tests and procedures that will be taken to ensure the integrity of the code as well as to ensure that
implementation meets the requirements described in the Software Requirements and Specifications (SRS) document (\cite{SRS}).
\section{General Information}
This section provides general information about the \ProjectName{}project.
\subsection{Summary}

The \ProjectName{}project will involve constructing a library that enables users to both
train and test multi-sensor 3D object detection networks on public autonomous driving datasets. 
Expanding upon the capabilities of the OpenPCDet repository (\cite{openpcdet2020}), it will serve as a foundation for 
further research into the field by presenting a method for LiDAR-Camera fusion-based object 
detection that better addresses alignment issues faced by prior methods. 

The training and inference (testing) portions of \ProjectName{}correspond to two different use-cases 
that require different considerations. During training, the library is intended to allow an experienced
machine learning and perception engineer to train a LiDAR-Camera fusion network on an autonomous driving 
dataset of their choosing. During inference, the library is intended to allow a user or a greater system 
to predict the set of dynamic objects in an autonomous vehicle's surroundings using a trained model and 
sensor input.

\subsection{Objectives}

The primary objectives is to build confidence in the correctness of the software.
As a routinely used and validated open-source repository, the base functionality of the OpenPCDet
repository (\cite{openpcdet2020}) is considered to be error-free and thus its correctness is outside the scope of this document/project.
The installation of the software and the extensions made to it are the components for which correctness will be 
verified. 

Verifying the accuracy of \ProjectName{}relative to existing baselines implemented in 
OpenPCDet (\cite{openpcdet2020}) is the secondary objective. Since the software is intended to serve as a foundation
for future research, its relative accuracy compared to state-of-the-art baseline methods will serve 
as a metric to judge how much it contributes to research in the field.

A tertiary objective is ensuring the usability of the software. Since the library will be extended and
used for research purposes, it is important to minimize the effort needed to understand and modify the code
as needed. 

\subsection{Relevant Documentation}

The other relevant design documents as well as their relations to this document are listed as follows:

\begin{itemize}
  \item SRS (\cite{SRS}): Describes the problem the software solves, the software's goals, 
  the relevant theoretical models, and the requirements for potential solutions. This document 
  thereby establishes the setting and mathematical tools necessary for the validation and verification
  methods described in this document.
  \item More documents will be added as they are completed.
\end{itemize}

\section{Plan}

This section describes the approaches that will be taken to verify and validate the different documents
and steps involved in the design process. Namely, it will state the members of the VnV team and will describe
the steps that will be taken to verify the SRS (\cite{SRS}), design, VnV plan, implementation. It will also describe the automating
testing tools that will be used.

\subsection{Verification and Validation Team}

\begin{table}[H]
  \centering
  \begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
  \hline
  \textbf{Name} & \textbf{Role} & \textbf{Description} & \textbf{Deliverables}\\ \hline
  Alaap Grandhi & Author & Write the VnV plan, carry out the testing detailed in it, fill in the VnV report, and validate the software and documents against the requirements during development. & PS\&G, SRS, VnV plan, MG, MIS, Updated SRS, Updated VnV plan, Updated MG, Updated MIS, VnV report, Reflect and Trace document, Code\\ \hline
  Spencer Smith & Project Supervisor/Course Instructor & Review and provide feedback on the VnV plan and report. Additionally, provide insight into better testing methods. & PS\&G feedback, SRS feedback, VnV plan feedback, MG feedback, MIS feedback\\ \hline
  Matthew Giamou & Masters Supervisor & Provide advice on the project in general. & SRS Comments/Guidance, MIS Comments/Guidance, Code Guidance and Suggestions \\ \hline
  Bo Liang & Domain Expert Reviewer & Review and provide feedback on the VnV plan and report. & VnV plan github issues feedback, MG github issues feedback, MIS github issues feedback\\ \hline 
  \end{tabular}
  \caption{VnV team members and their responsibilities}
  \label{Table:VnV_Team}
\end{table}

\subsection{SRS Verification Plan}

The SRS will be verified in a manner that is inspired and informed by Liu (insert cite here).
Specifically, I will verify my SRS through the following steps:

First, I will review my SRS document as a whole by doing the following:
\begin{enumerate}
  \item Going through the document using the \href{https://github.com/alaapgrandhi/equivariant-sensor-fusion/blob/main/docs/Checklists/SRS-Checklist.pdf}{SRS checklist} on my own and taking my own set of notes
  \item Sharing the document with my Domain Expert and allowing them to create their own notes
  \item Sitting down with my Domain Expert and asking a set of open-ended questions to my Domain Expert to see if the document was clear to them:
  \begin{itemize}
    \item What general problem do you think SRS aims to address?
    \item How could the object detection task focused on in the SRS aid Autonomous Driving?
    \item What is your understanding of the difference between the training and inference pipelines?
    \item What do you think some important characteristics of a solution would be here?
    \item How did the mathematical notation used in the SRS affect the reading experience?
    \item Beyond these, other questions will be asked based on their responses to the above
  \end{itemize}
  \item Comparing our notes on the document to create unified more complete set of notes
\end{enumerate}
By separating my note-taking process from my Domain Expert's note-taking process, I will not 
run the risk of accidentally influencing or biasing their understanding of the document with my 
own thought process. This way, I can get a better idea of the SRS's merit as a standalone document.


Following this high-level review, I plan on going through the SRS line-by-line with my Masters Supervisor 
(Dr. Matthew Giamou). As someone with a strong history of publishing robotics-focused research, he should be 
able to guide me on ways to make my SRS more useful for the research community I am targeting (since this project 
aims to empower future research). I will center this line-by-line review around the following questions:
\begin{enumerate}
  \item Does the mathematical notation used align with standards in the field?
  \item Does the problem outlined in the introduction reflect an actual gap in the field?
  \item Do the project scope and system constraints seem reasonable given past research in the field?
  \item Would it be reasonable to assume that the average computer-vision robotics conference attendee possesses the characteristics of the intended reader?
  \item Do the training and inference pipelines outlined in the general system description seem reasonable?
  \item Could you see researchers using this software?
  \item Is all of the terminology defined correctly?
  \item Do the assumptions made generally hold for autonomous robotics?
  \item Do the mathematical models presented seen correct?
  \item Are each of the instance models presented relevant to the task at hand?
  \item Is the bounding box loss function a sufficient signal for training the model?
  \item Does the mAP IOU-based evaluation metric accurately reflect the usefulness of the software?
  \item Do the functional requirements align with prior research in the field?
  \item Are the nonfunctional requirements sufficient for gauging the quality of the software?
  \item Are there any forseeable likely or unlikely changes that I seem to be missing?
\end{enumerate}


Once both of these have been conducted, I plan on taking suggestions for changes from my Domain Expert 
to refine my document. Specifically, this feedback will be provided in the form of Github issues. Following 
the creation of such issues, I will review the document with these suggestions in mind and will make changes 
where appropriate (or explain why such a change is unnecessary if not). Then, the Github issues will be responded to and closed. 
Due to time constraints, a prototype will not be used for SRS verification. 

\subsection{Design Verification Plan}

To verify the design, I have constructed the following checklist detailing the aspects of the design
that the Domain Expert Reviewer and optionally the Masters and Project Supervisors (if they have time) 
will evaluate during regular discussions:
\begin{itemize}
  \item Does the design explicitly or implicitly address all requirements described in the SRS document? (\cite{SRS})
  \item Are the interfaces between modules of the design clear and in-line with the modular structure of the OpenPCDet repository? (\cite{openpcdet2020})
  \item Given the overall design description, can the Domain Expert Reviewer accurately roughly predict the downstream code structure?
  \item Are any aspects of the design ambiguous?
  \item Does the design leverage existing functionality from the OpenPCDet repository (\cite{openpcdet2020}) where applicable?
  \item Do all of the design components contribute towards meeting a requirement?
  \begin{itemize}
    \item If not, is this due to a redundancy in the design or something missing in the SRS document? (\cite{SRS})
  \end{itemize}
\end{itemize}

\subsection{Verification and Validation Plan Verification Plan}

Manual tests and the overall structure of the VnV plan will be reviewed by the Project Supervisor
and the Domain Expert Reviewer. This will be done to both ensure that the document adheres to the
\href{https://github.com/alaapgrandhi/equivariant-sensor-fusion/blob/main/docs/Checklists/VnV-Checklist.pdf}{template} and to ensure that the tests presented sufficiently cover the requirements described in
the SRS document (\cite{SRS}). Automated tests will be verified by mutation testing where possible.

\subsection{Implementation Verification Plan} \label{imple_ver}

The implementation will simply be verified through semi-regular code walkthroughs with my Domain Expert.
These walkthroughs will consist of me going through my MIS modules one-by-one and displaying these side-by-side with the
code sections corresponding to them. By explaining the code that is meant to realize each module in a step-by-step manner, 
my Domain Expert will be able to tell me if my code seems incorrect or does not match its specification at any point.
Additionally, the tests described in Section \ref{System_Tests} will serve as a basis for verifying that the 
implementation meets the requirements detailed in the SRS document. (\cite{SRS})

\subsection{Automated Testing and Verification Tools}

There are a few different automated tools that will be used for testing.
Since the library will be written in Python (the OpenPCDet library (\cite{openpcdet2020})
is written in Python), PyFlakes will be used as an error linter to discover
simple logical errors. Linters like flake8 that enforce style convention will
not be used as the base OpenPCDet repository (\cite{openpcdet2020}) does not adhere to any particular
style convention (it is designed with research and quick development as the primary
objective) and it would be out of scope to refactor that. Additionally, the automated
unit tests below will be implemented in pytest and will be set to automatically run
on push using github actions. Code coverage will not be considered for this project
since by nature different components are meant to be called according to the training
configuration passed in and it would overly tedious to routinely test over all possible
input configurations.

\subsection{Software Validation Plan}

In order to validate my Software, I plan on running it over the evaluation set of NuScenes and 
saving a set of visualizations for each input. Specifically, these visualizations will display
input pointclouds alongside predicted bounding boxes. From there, my Domain Expert and I will visually
inspect these to see if they provide a sufficient and accurate representation of the objects in the Autonomous 
Vehicle's surroundings. Validation of the requirements themselves will only 
briefly be covered during discussions with my Masters Supervisor.
\section{System Tests} \label{System_Tests}

This section details a set of tests that shall be sufficient to ensure that \ProjectName{}meets the requirements detailed in the SRS document.
Additional tests may be added as the SRS document is refined.

\subsection{Tests for Functional Requirements}
The subsections below detail the tests that will be used to ensure that \ProjectName{}meets the functional requirements.

\subsubsection{Input Format Testing}

These tests will ensure that in inference, the system is able to process all the necessary
file formats for the camera images and LiDAR pointclouds.
		
\paragraph{Camera Image Input Testing}

\begin{enumerate}

\item{test-camera-valid1$\rightarrow$4\\}

Control: Automatic
					
Initial State: The inference pipeline initialized with a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pre-defined trained model} and \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} loaded. A clean, conforming LiDAR pointcloud in the pcd.bin format
from the NuScenes dataset is also expected to have already been loaded (this will be handpicked from NuScenes/v1.0-trainval/samples/LIDAR\_TOP and put into the test's directory).
					
Input: A valid (subject to input constraints) multiview camera image set stored in each of the file formats specified in requirement R1 (one test case for file format).
					
Output: The inference pipeline should run as expected without errors and output a set of predicted bounding boxes. The actual values for these bounding boxes do not matter as long as the shape of the output is as expected and the constraints on the values (i.e. height of a bounding box is non-negative) hold.

Test Case Derivation: For all the file formats specified in requirement R1, the inference pipeline should run without issues. All other aspects are kept
constant between the tests to ensure that the test can detect any issues with specific image file formats.
					
How test will be performed: This test will be automatically run through pytest. The configuration, pretrained model, LiDAR pointcloud will all be manually generated beforehand
and then automatically accessed by pytest. The same set of valid multiview camera images will simply be converted to the different file types beforehand to be automatically accessed
by pytest on github push.
					
\item{test-camera-invalid\\}

Control: Automatic
					
Initial State: The inference pipeline initialized with a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pre-defined trained model} and \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} loaded. A clean, conforming LiDAR pointcloud in the pcd.bin format
from the NuScenes dataset is also expected to have already been loaded (this will be handpicked from NuScenes/v1.0-trainval/samples/LIDAR\_TOP and put into the test's directory).
					
Input: A valid (subject to input constraints) multiview camera image set stored in a file format that is not expected to be handled by the program (a binary image).
					
Output: A invalid-input error is expected to be thrown by the program.

Test Case Derivation: The inference pipeline should not even try to run for any file format not specified in requirement R1. This is to test for cases
where images are input in unforeseen or invalid formats, and makes sure that the system's response to such input types is well-defined. 

How test will be performed: This test will be automatically run through pytest on github push in much the same way as the valid tests. The only difference is that it will
load a set of multiview camera images stored in a binary format.

\end{enumerate}

\paragraph{LiDAR Pointcloud Input Testing}

\begin{enumerate}

\item{test-lidar-valid1$\rightarrow$4\\}

Control: Automatic
					
Initial State: The inference pipeline initialized with a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pre-defined trained model} and \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} loaded. A clean, conforming set of multiview camera images in the JPEG format
from the NuScenes Dataset is also expected to have already been loaded (this set will be handpicked from NuScenes/v1.0-trainval/samples/CAM\_* with one corresponding image from each CAM folder).
					
Input: A valid (subject to input constraints) LiDAR pointcloud stored in each of the file formats specified in requirement R2 (one test case for each format).
					
Output: The inference pipeline should run as expected without errors and output a set of predicted bounding boxes. The actual values for these bounding boxes do not matter as long as the shape of the output is as expected and the constraints on the values (i.e. height of a bounding box is non-negative) hold.

Test Case Derivation: For all the file formats specified in requirement R2, the inference pipeline should run without issues. All other aspects are kept
constant between the tests to ensure that the test can detect any issues with specific image file formats.
					
How test will be performed: This test will be automatically run through pytest. The configuration, pretrained model, multiview camera image set will all be manually generated beforehand
and then automatically accessed by pytest. The same LiDAR pointcloud will simply be converted to the different file types beforehand to be automatically accessed
by pytest on github push.
					
\item{test-lidar-invalid\\}

Control: Automatic
					
Initial State: The inference pipeline initialized with a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pre-defined trained model} and \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} loaded. A clean, conforming set of multiview camera images in the JPEG format
from the NuScenes Dataset is also expected to have already been loaded (this set will be handpicked from NuScenes/v1.0-trainval/samples/CAM\_* with one corresponding image from each CAM folder).
					
Input: A LiDAR pointcloud stored in the binary file format (since this format is not one of the ones specified in R2).
					
Output: A invalid-input error is expected to be thrown by the program.

Test Case Derivation: The inference pipeline should not even try to run for any file format not specified in requirement R2. This is to test for cases
where pointclouds are input in unforeseen or invalid formats, and makes sure that the system's response to such input types is well-defined. 

How test will be performed: This test will be automatically run through pytest on github push in much the same way as the valid tests. The only difference is that it will
load a LiDAR pointcloud stored in a binary format.

\end{enumerate}

\subsubsection{Tests for Correct Optimization}

The test below will ensure that \ProjectName{}is being optimized correctly.

\paragraph{Model Training Consistency Testing}
\begin{enumerate}

  \item{test-consistent\\}
  
  Control: Manual
            
  Initial State: A set of pre-configured baseline methods will be set up for training. Additionally, the proposed method built on the OpenPCDet (\cite{openpcdet2020}) platform
  will be set up for training.
            
  Input: A sufficiently small training dataset that has been predetermined to produce similar training accuracies across the baseline methods 
  (corresponding to an mAP standard deviation below a threshold that will be decided on with my Domain Expert).
            
  Output: When all the models are trained and subsequently evaluated on the same small training dataset, the proposed method shall not fall below the mean mAP of
  the baseline methods by more than a given absolute threshold that will be decided on with my Domain Expert.
  
  Test Case Derivation: There is no true way to test if the combined bounding box is exactly minimized considering the randomness inherent in machine learning techniques
  and floating point computation. Rather, since the baselines are assumed to roughly minimize the bounding box loss on the training dataset, consistency with their results 
  should be sufficient for verifying this. Additionally, by evaluating on the same dataset that was trained on, we can ensure that the ADAM optimization is minimizing the loss
  for the training subset.
            
  How test will be performed: This test will be manually run due to the time-intensive nature of model training. Moreover, it will not be regularly run but rather only run when 
  major changes to \ProjectName{} are made. It will also not be run until the model is in a trainable state as a whole and the other tests have all passed.
\end{enumerate}

Note that since ADAM will be directly imported from pytorch's implementation, its validity will not be verified. Rather, the code walkthroughs mentioned above in section \ref{imple_ver}
will aid in verifying that it has been used correctly.

\subsubsection{Tests for Visualization}

The below test will verify that the visualization produced during the inference phase is correct.

\paragraph{Visualization Testing}
\begin{enumerate}

  \item{test-viz-correct\\}
  
  Control: Automatic
            
  Initial State: Any components aside from the inputs that are needed for the visualization pipeline to run.
            
  Input: A set of pre-configured bounding boxes will be loaded alongside a predetermine LiDAR pointcloud. Additionally, a set of viewing positions and resolutions
  to view the produced pointclouds from.

  Output: The output pointcloud visualizations with bounding boxes should look identical to pre-generated verified visualizations of the same scene and bounding boxes. These
  pre-generated visualizations will be created in the future and carefully examined to ensure correctness before setting up the test.
  
  Test Case Derivation: This is really just using the method of manufactured solutions to verify that the visualization pipeline is working as expected without bugs. A correct
  and matching visualization here does not guarantee a correctly functioning visualization pipeline but it is a necessary condition.
            
  How test will be performed: This test will automatically be run through pytest with the LiDAR pointcloud, bounding boxes, and verified visualizations being generated beforehand.
  Matplotlib.testing.compare.compare\_images will be run to compare each generated visualization with the corresponding verified visualization.
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}
The subsections below will detail the tests that will ensure that \ProjectName{}meets the nonfunctional requirements.

\subsubsection{Tests for Accuracy}

The test below will ensure that \ProjectName{}is at least relatively as accurate as the BEVFusion baseline (\cite{liang2022bevfusion}).
		
\paragraph{Accuracy Testing}

\begin{enumerate}

\item{test-mAP\\}

Type: Manual
					
Initial State: The BEVFusion model will be loaded with the BEVFusion configuration currently present in the OpenPCDet repository (\cite{openpcdet2020}).
Additionally, \ProjectName{}will be loaded with some chosen configuration.
					
Input/Condition: A training and validation set from the nuScenes dataset (\cite{caesar2020nuscenes}).
					
Output/Result: After both models have been trained on the nuScenes training set (\cite{caesar2020nuscenes}), their mAP metrics on the validation set will be returned. The
proposed solution's validation mAP should not fall below the BEVFusion mAP by more than a threshold that will be determined with my Domain Expert.
					
How test will be performed: Since model training is time-intensive, this test will mainly be conducted whenever major changes to \ProjectName{}are made.
The threshold will only serve as a guideline and after each run of this test, a discussion will be done with my Domain Expert and Masters Supervisor to determine
if the result is satisfactory.

\end{enumerate}

While Waymo (\cite{sun2020scalability})could be tested for as well, it is a much larger dataset that would take quite long to train on. Moreover, evaluating this test for the nuScenes dataset (\cite{caesar2020nuscenes})
alone should provide almost as much insight as evaluating on both (since in existing research, better performing models tend to do better across both datasets).

\subsubsection{Tests for Understandability}

\paragraph{Understandability Testing}

Rather than use a fixed test for understandability, it will be tested closer to the end of the project through code walkthroughs. Specifically,
I will provide a demonstration of how to use the library to my Domain Expert and optionally Project and Masters supervisors (if they have time).
I will ask them the following questions afterwords to gauge how easy to understand the code is.
\begin{enumerate}
  \item How do you think the code works at a high level?
  \item If you had to describe what $x$ (to be determined after implementation) class of submodules does, how would you do so?
  \item If you had to learn how to use this code with nothing but the documentation present in the repository and the code itself, how long do you think it would take you to do so?
\end{enumerate}

Afterwords, I will test further test their understanding of the code by letting them play around with it and getting them to perform the following tasks with assistance as necessary.
\begin{enumerate}
  \item Train a model using $x$ (to be determined after implementation) configuration.
  \item Evaluate a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pretrained model} on the nuScenes validation dataset (\cite{caesar2020nuscenes}).
  \item Modify the \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} for this pretrained model to change the post-processing prediction range.
\end{enumerate}

Through the code walkthrough and their subsequent interaction with it, I hope to get at least a rough idea of how understandable the code is.

\subsubsection{Tests for Installability}

\paragraph{Installability Testing}

Since I implemented the code, it would not really make sense to test the installability of \progname{} myself.
This is because I am familiar with the bugs that I faced when initially installing the code and thus would be able to 
troubleshoot on the fly where the user might not be able to. Rather, I plan to sit down with either my domain expert 
or Masters Supervisor (Dr. Matthew Giamou) and have them perform the following steps with minimal guidance:

\begin{enumerate}
  \item Clone the repository from github to get a fresh copy by running "git clone URL"
  \item Create a conda environment from the provided environment.yml file by running "conda env create -f environment.yml" from the environment subfolder
  \item Setup the repository using OpenPCDet's setup.py file by running "python setup.py develop" from the base directory
  \item Follow OpenPCDet's dataset setup guide for the NuScenes Dataset as explained \href{https://github.com/open-mmlab/OpenPCDet/blob/master/docs/GETTING_STARTED.md}{here}
  \begin{enumerate}
    \item Since the full dataset is too large, I would tell them to install the mini version instead
    \item If they do not want to have to create a NuScenes account to download the dataset, they can simply download the mini set by running "wget https://d36yt3mvayqw5m.cloudfront.net/public/v1.0/v1.0-mini.tgz"
    \item The pip install step in OpenPCDet's dataset setup guide should be ignored as it is already installed in the conda environment
    \item The multi-modal data infos should be generated rather than the lidar-only infos
  \end{enumerate}
  \item Install the \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pretrained BEVFusion model} and place it in the pretrained\_models folder
  \item Install the \href{https://drive.google.com/file/d/1v74WCt4_5ubjO7PciA5T0xhQc9bz_jZu/view}{Pretrained NuImages Swin-Transformer} and place it in the pretrained\_models folder
  \item Evaluate this pretrained model on the nuScenes mini validation dataset (\cite{caesar2020nuscenes}) (I will add a guide for this in the Github's README file)
\end{enumerate}

If they are able to perform all these steps without errors (or with minimal errors that a researcher in the field would be able to troubleshoot),
then I would consider this test to have passed. Otherwise, the steps that fail would provide insight into how the installability of the software could be improved.

\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
    & R1 & R2 & R3 & R4 & NFR1 & NFR2 \\
  \hline
  Camera Image Input Testing             &X& & & & & \\ \hline
  LiDAR Pointcloud Input Testing         & &X& & & & \\ \hline
  Model Training Consistency Testing     & & &X& & & \\ \hline
  Visualization Testing                  & & & &X& & \\ \hline
  Accuracy Testing                       & & & & &X& \\ \hline
  Understandability Testing              & & & & & &X\\ \hline
  \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between Requirements and Tests}
  \label{Table:R_trace}
  \end{table}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\end{document}