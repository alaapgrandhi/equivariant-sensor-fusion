\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{float}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{pdflscape}
\usepackage{xr-hyper}
\externaldocument[MIS-]{../Design/SoftDetailedDes/MIS}

\input{../Comments}
\input{../Common}

\newcommand{\ProjectName}{ESF }

\begin{document}

\title{System Verification and Validation Plan for \ProjectName{}} 
\author{Alaap Grandhi}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
February 27 & 1.0 & Initial version of the VnV Plan\\
April 17 & 2.0 & Updated according to Professor Smith's feedback\\
April 20 & 3.0 & Added Unit Testing \\
\bottomrule
\end{tabularx}

\newpage

\tableofcontents

\listoftables

\newpage

\pagenumbering{arabic}

This document will describe the Verification and Validation (VnV) plan for the \ProjectName{}project. It will
outline the tests and procedures that will be taken to ensure the integrity of the code as well as to ensure that
implementation meets the requirements described in the Software Requirements and Specifications (SRS) document (\cite{SRS}).
\section{General Information}
This section provides general information about the \ProjectName{}project.
\subsection{Summary}

The \ProjectName{}project will involve constructing a library that enables users to both
train and test multi-sensor 3D object detection networks on public autonomous driving datasets. 
Expanding upon the capabilities of the OpenPCDet repository (\cite{openpcdet2020}), it will serve as a foundation for 
further research into the field by presenting a method for LiDAR-Camera fusion-based object 
detection that better addresses alignment issues faced by prior methods. 

The training and inference (testing) portions of \ProjectName{}correspond to two different use-cases 
that require different considerations. During training, the library is intended to allow an experienced
machine learning and perception engineer to train a LiDAR-Camera fusion network on an autonomous driving 
dataset of their choosing. During inference, the library is intended to allow a user or a greater system 
to predict the set of dynamic objects in an autonomous vehicle's surroundings using a trained model and 
sensor input.

\subsection{Objectives}

The primary objectives is to build confidence in the correctness of the software.
As a routinely used and validated open-source repository, the base functionality of the OpenPCDet
repository (\cite{openpcdet2020}) is considered to be error-free and thus its correctness is outside the scope of this document/project.
The installation of the software and the extensions made to it are the components for which correctness will be 
verified. 

Verifying the accuracy of \ProjectName{}relative to existing baselines implemented in 
OpenPCDet (\cite{openpcdet2020}) is the secondary objective. Since the software is intended to serve as a foundation
for future research, its relative accuracy compared to state-of-the-art baseline methods will serve 
as a metric to judge how much it contributes to research in the field.

A tertiary objective is ensuring the usability of the software. Since the library will be extended and
used for research purposes, it is important to minimize the effort needed to understand and modify the code
as needed. 

\subsection{Relevant Documentation}

The other relevant design documents as well as their relations to this document are listed as follows:

\begin{itemize}
  \item SRS (\cite{SRS}): Describes the problem the software solves, the software's goals, 
  the relevant theoretical models, and the requirements for potential solutions. This document 
  thereby establishes the setting and mathematical tools necessary for the validation and verification
  methods described in this document.
  \item More documents will be added as they are completed.
\end{itemize}

\section{Plan}

This section describes the approaches that will be taken to verify and validate the different documents
and steps involved in the design process. Namely, it will state the members of the VnV team and will describe
the steps that will be taken to verify the SRS (\cite{SRS}), design, VnV plan, implementation. It will also describe the automating
testing tools that will be used.

\subsection{Verification and Validation Team}

\begin{table}[H]
  \centering
  \begin{tabular}{|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
  \hline
  \textbf{Name} & \textbf{Role} & \textbf{Description} & \textbf{Deliverables}\\ \hline
  Alaap Grandhi & Author & Write the VnV plan, carry out the testing detailed in it, fill in the VnV report, and validate the software and documents against the requirements during development. & PS\&G, SRS, VnV plan, MG, MIS, Updated SRS, Updated VnV plan, Updated MG, Updated MIS, VnV report, Reflect and Trace document, Code\\ \hline
  Spencer Smith & Project Supervisor/Course Instructor & Review and provide feedback on the VnV plan and report. Additionally, provide insight into better testing methods. & PS\&G feedback, SRS feedback, VnV plan feedback, MG feedback, MIS feedback\\ \hline
  Matthew Giamou & Masters Supervisor & Provide advice on the project in general. & SRS Comments/Guidance, MIS Comments/Guidance, Code Guidance and Suggestions \\ \hline
  Bo Liang & Domain Expert Reviewer & Review and provide feedback on the VnV plan and report. & VnV plan github issues feedback, MG github issues feedback, MIS github issues feedback\\ \hline 
  \end{tabular}
  \caption{VnV team members and their responsibilities}
  \label{Table:VnV_Team}
\end{table}

\subsection{SRS Verification Plan}

The SRS will be verified in a manner that is inspired and informed by Liu (insert cite here).
Specifically, I will verify my SRS through the following steps:

First, I will review my SRS document as a whole by doing the following:
\begin{enumerate}
  \item Going through the document using the \href{https://github.com/alaapgrandhi/equivariant-sensor-fusion/blob/main/docs/Checklists/SRS-Checklist.pdf}{SRS checklist} on my own and taking my own set of notes
  \item Sharing the document with my Domain Expert and allowing them to create their own notes
  \item Sitting down with my Domain Expert and asking a set of open-ended questions to my Domain Expert to see if the document was clear to them:
  \begin{itemize}
    \item What general problem do you think SRS aims to address?
    \item How could the object detection task focused on in the SRS aid Autonomous Driving?
    \item What is your understanding of the difference between the training and inference pipelines?
    \item What do you think some important characteristics of a solution would be here?
    \item How did the mathematical notation used in the SRS affect the reading experience?
    \item Beyond these, other questions will be asked based on their responses to the above
  \end{itemize}
  \item Comparing our notes on the document to create unified more complete set of notes
\end{enumerate}
By separating my note-taking process from my Domain Expert's note-taking process, I will not 
run the risk of accidentally influencing or biasing their understanding of the document with my 
own thought process. This way, I can get a better idea of the SRS's merit as a standalone document.


Following this high-level review, I plan on going through the SRS line-by-line with my Masters Supervisor 
(Dr. Matthew Giamou). As someone with a strong history of publishing robotics-focused research, he should be 
able to guide me on ways to make my SRS more useful for the research community I am targeting (since this project 
aims to empower future research). I will center this line-by-line review around the following questions:
\begin{enumerate}
  \item Does the mathematical notation used align with standards in the field?
  \item Does the problem outlined in the introduction reflect an actual gap in the field?
  \item Do the project scope and system constraints seem reasonable given past research in the field?
  \item Would it be reasonable to assume that the average computer-vision robotics conference attendee possesses the characteristics of the intended reader?
  \item Do the training and inference pipelines outlined in the general system description seem reasonable?
  \item Could you see researchers using this software?
  \item Is all of the terminology defined correctly?
  \item Do the assumptions made generally hold for autonomous robotics?
  \item Do the mathematical models presented seen correct?
  \item Are each of the instance models presented relevant to the task at hand?
  \item Is the bounding box loss function a sufficient signal for training the model?
  \item Does the mAP IOU-based evaluation metric accurately reflect the usefulness of the software?
  \item Do the functional requirements align with prior research in the field?
  \item Are the nonfunctional requirements sufficient for gauging the quality of the software?
  \item Are there any forseeable likely or unlikely changes that I seem to be missing?
\end{enumerate}


Once both of these have been conducted, I plan on taking suggestions for changes from my Domain Expert 
to refine my document. Specifically, this feedback will be provided in the form of Github issues. Following 
the creation of such issues, I will review the document with these suggestions in mind and will make changes 
where appropriate (or explain why such a change is unnecessary if not). Then, the Github issues will be responded to and closed. 
Due to time constraints, a prototype will not be used for SRS verification. 

\subsection{Design Verification Plan}

To verify the design, I have constructed the following checklist detailing the aspects of the design
that the Domain Expert Reviewer and optionally the Masters and Project Supervisors (if they have time) 
will evaluate during regular discussions:
\begin{itemize}
  \item Does the design explicitly or implicitly address all requirements described in the SRS document? (\cite{SRS})
  \item Are the interfaces between modules of the design clear and in-line with the modular structure of the OpenPCDet repository? (\cite{openpcdet2020})
  \item Given the overall design description, can the Domain Expert Reviewer accurately roughly predict the downstream code structure?
  \item Are any aspects of the design ambiguous?
  \item Does the design leverage existing functionality from the OpenPCDet repository (\cite{openpcdet2020}) where applicable?
  \item Do all of the design components contribute towards meeting a requirement?
  \begin{itemize}
    \item If not, is this due to a redundancy in the design or something missing in the SRS document? (\cite{SRS})
  \end{itemize}
\end{itemize}

\subsection{Verification and Validation Plan Verification Plan}

Manual tests and the overall structure of the VnV plan will be reviewed by the Project Supervisor
and the Domain Expert Reviewer. This will be done to both ensure that the document adheres to the
\href{https://github.com/alaapgrandhi/equivariant-sensor-fusion/blob/main/docs/Checklists/VnV-Checklist.pdf}{template} and to ensure that the tests presented sufficiently cover the requirements described in
the SRS document (\cite{SRS}). Automated tests will be verified by mutation testing where possible.

\subsection{Implementation Verification Plan} \label{imple_ver}

The implementation will simply be verified through semi-regular code walkthroughs with my Domain Expert.
These walkthroughs will consist of me going through my MIS modules one-by-one and displaying these side-by-side with the
code sections corresponding to them. By explaining the code that is meant to realize each module in a step-by-step manner, 
my Domain Expert will be able to tell me if my code seems incorrect or does not match its specification at any point.
Additionally, the tests described in Section \ref{System_Tests} will serve as a basis for verifying that the 
implementation meets the requirements detailed in the SRS document. (\cite{SRS})

\subsection{Automated Testing and Verification Tools}

There are a few different automated tools that will be used for testing.
Since the library will be written in Python (the OpenPCDet library (\cite{openpcdet2020})
is written in Python), PyFlakes will be used as an error linter to discover
simple logical errors. Linters like flake8 that enforce style convention will
not be used as the base OpenPCDet repository (\cite{openpcdet2020}) does not adhere to any particular
style convention (it is designed with research and quick development as the primary
objective) and it would be out of scope to refactor that. Additionally, the automated
unit tests below will be implemented in pytest and will be set to automatically run
on push using github actions. Code coverage will not be considered for this project
since by nature different components are meant to be called according to the training
configuration passed in and it would overly tedious to routinely test over all possible
input configurations.

\subsection{Software Validation Plan}

In order to validate my Software, I plan on running it over the evaluation set of NuScenes and 
saving a set of visualizations for each input. Specifically, these visualizations will display
input pointclouds alongside predicted bounding boxes. From there, my Domain Expert and I will visually
inspect these to see if they provide a sufficient and accurate representation of the objects in the Autonomous 
Vehicle's surroundings. Validation of the requirements themselves will only 
briefly be covered during discussions with my Masters Supervisor.
\section{System Tests} \label{System_Tests}

This section details a set of tests that shall be sufficient to ensure that \ProjectName{}meets the requirements detailed in the SRS document.
Additional tests may be added as the SRS document is refined.

\subsection{Tests for Functional Requirements}
The subsections below detail the tests that will be used to ensure that \ProjectName{}meets the functional requirements.

\subsubsection{Input Format Testing} \label{Input Format Testing}

These tests will ensure that in inference, the system is able to process all the necessary
file formats for the camera images and LiDAR pointclouds.
		
\paragraph{Camera Image Input Testing}

\begin{enumerate}

\item{test-camera-valid1$\rightarrow$4\\}

Control: Automatic
					
Initial State: The inference pipeline initialized with a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pre-defined trained model} and \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} loaded. A clean, conforming LiDAR pointcloud in the pcd.bin format
from the NuScenes dataset is also expected to have already been loaded (this will be handpicked from NuScenes/v1.0-trainval/samples/LIDAR\_TOP and put into the test's directory).
					
Input: A valid (subject to input constraints) multiview camera image set stored in each of the file formats specified in requirement R1 (one test case for file format).
					
Output: The inference pipeline should run as expected without errors and output a set of predicted bounding boxes. The actual values for these bounding boxes do not matter as long as the shape of the output is as expected and the constraints on the values (i.e. height of a bounding box is non-negative) hold.

Test Case Derivation: For all the file formats specified in requirement R1, the inference pipeline should run without issues. All other aspects are kept
constant between the tests to ensure that the test can detect any issues with specific image file formats.
					
How test will be performed: This test will be automatically run through pytest. The configuration, pretrained model, LiDAR pointcloud will all be manually generated beforehand
and then automatically accessed by pytest. The same set of valid multiview camera images will simply be converted to the different file types beforehand to be automatically accessed
by pytest on github push.
					
\item{test-camera-invalid\\}

Control: Automatic
					
Initial State: The inference pipeline initialized with a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pre-defined trained model} and \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} loaded. A clean, conforming LiDAR pointcloud in the pcd.bin format
from the NuScenes dataset is also expected to have already been loaded (this will be handpicked from NuScenes/v1.0-trainval/samples/LIDAR\_TOP and put into the test's directory).
					
Input: A valid (subject to input constraints) multiview camera image set stored in a file format that is not expected to be handled by the program (a binary image).
					
Output: A invalid-input error is expected to be thrown by the program.

Test Case Derivation: The inference pipeline should not even try to run for any file format not specified in requirement R1. This is to test for cases
where images are input in unforeseen or invalid formats, and makes sure that the system's response to such input types is well-defined. 

How test will be performed: This test will be automatically run through pytest on github push in much the same way as the valid tests. The only difference is that it will
load a set of multiview camera images stored in a binary format.

\item{test-camera-small\\}

Control: Automatic
					
Initial State: The inference pipeline initialized with a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pre-defined trained model} and \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} loaded. A clean, conforming LiDAR pointcloud in the pcd.bin format
from the NuScenes dataset is also expected to have already been loaded (this will be handpicked from NuScenes/v1.0-trainval/samples/LIDAR\_TOP and put into the test's directory).
					
Input: A multiview camera image set with a height of one and a width of one (too small) stored in the valid JPEG format.
					
Output: A invalid-input error is expected to be thrown by the program.

Test Case Derivation: The inference pipeline should not even try to run for an image that is does not match the input constraint. This is to test for cases
where improperly-sized images are input, and makes sure that the system's response to such input types is well-defined. 

How test will be performed: This test will be automatically run through pytest on github push in much the same way as the valid tests. The only difference is that it will
load a set of multiview camera images of size 1x1.

\item{test-camera-wrongnumviews\\}

Control: Automatic
					
Initial State: The inference pipeline initialized with a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pre-defined trained model} and \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} loaded. A clean, conforming LiDAR pointcloud in the pcd.bin format
from the NuScenes dataset is also expected to have already been loaded (this will be handpicked from NuScenes/v1.0-trainval/samples/LIDAR\_TOP and put into the test's directory).
					
Input: A multiview camera image set with ten views (where NuScenes expects six) stored in the valid JPEG file format.
					
Output: A invalid-input error is expected to be thrown by the program.

Test Case Derivation: The inference pipeline should not even try to run for a multiview image with a different number of views than the training dataset (NuScenes). This is to test for cases
where the wrong amount of image views are input, and makes sure that the system's response to such input types is well-defined. 

How test will be performed: This test will be automatically run through pytest on github push in much the same way as the valid tests. The only difference is that it will
load a set of multiview camera images with ten views instead of the normal six.

\end{enumerate}

\paragraph{LiDAR Pointcloud Input Testing}

\begin{enumerate}

\item{test-lidar-valid1$\rightarrow$4\\}

Control: Automatic
					
Initial State: The inference pipeline initialized with a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pre-defined trained model} and \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} loaded. A clean, conforming set of multiview camera images in the JPEG format
from the NuScenes Dataset is also expected to have already been loaded (this set will be handpicked from NuScenes/v1.0-trainval/samples/CAM\_* with one corresponding image from each CAM folder).
					
Input: A valid (subject to input constraints) LiDAR pointcloud stored in each of the file formats specified in requirement R2 (one test case for each format).
					
Output: The inference pipeline should run as expected without errors and output a set of predicted bounding boxes. The actual values for these bounding boxes do not matter as long as the shape of the output is as expected and the constraints on the values (i.e. height of a bounding box is non-negative) hold.

Test Case Derivation: For all the file formats specified in requirement R2, the inference pipeline should run without issues. All other aspects are kept
constant between the tests to ensure that the test can detect any issues with specific image file formats.
					
How test will be performed: This test will be automatically run through pytest. The configuration, pretrained model, multiview camera image set will all be manually generated beforehand
and then automatically accessed by pytest. The same LiDAR pointcloud will simply be converted to the different file types beforehand to be automatically accessed
by pytest on github push.
					
\item{test-lidar-invalid\\}

Control: Automatic
					
Initial State: The inference pipeline initialized with a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pre-defined trained model} and \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} loaded. A clean, conforming set of multiview camera images in the JPEG format
from the NuScenes Dataset is also expected to have already been loaded (this set will be handpicked from NuScenes/v1.0-trainval/samples/CAM\_* with one corresponding image from each CAM folder).
					
Input: A LiDAR pointcloud stored in the binary file format (since this format is not one of the ones specified in R2).
					
Output: A invalid-input error is expected to be thrown by the program.

Test Case Derivation: The inference pipeline should not even try to run for any file format not specified in requirement R2. This is to test for cases
where pointclouds are input in unforeseen or invalid formats, and makes sure that the system's response to such input types is well-defined. 

How test will be performed: This test will be automatically run through pytest on github push in much the same way as the valid tests. The only difference is that it will
load a LiDAR pointcloud stored in a binary format.

\end{enumerate}

\subsubsection{Tests for Correct Optimization}

The test below will ensure that \ProjectName{}is being optimized correctly.

\paragraph{Model Training Consistency Testing}
\begin{enumerate}

  \item{test-consistent\\}
  
  Control: Manual
            
  Initial State: A set of pre-configured baseline methods will be set up for training. Additionally, the proposed method built on the OpenPCDet (\cite{openpcdet2020}) platform
  will be set up for training.
            
  Input: A sufficiently small training dataset that has been predetermined to produce similar training accuracies across the baseline methods 
  (corresponding to an mAP standard deviation below a threshold that will be decided on with my Domain Expert).
            
  Output: When all the models are trained and subsequently evaluated on the same small training dataset, the proposed method shall not fall below the mean mAP of
  the baseline methods by more than a given absolute threshold that will be decided on with my Domain Expert.
  
  Test Case Derivation: There is no true way to test if the combined bounding box is exactly minimized considering the randomness inherent in machine learning techniques
  and floating point computation. Rather, since the baselines are assumed to roughly minimize the bounding box loss on the training dataset, consistency with their results 
  should be sufficient for verifying this. Additionally, by evaluating on the same dataset that was trained on, we can ensure that the ADAM optimization is minimizing the loss
  for the training subset.
            
  How test will be performed: This test will be manually run due to the time-intensive nature of model training. Moreover, it will not be regularly run but rather only run when 
  major changes to \ProjectName{} are made. It will also not be run until the model is in a trainable state as a whole and the other tests have all passed.
\end{enumerate}

Note that since ADAM will be directly imported from pytorch's implementation, its validity will not be verified. Rather, the code walkthroughs mentioned above in section \ref{imple_ver}
will aid in verifying that it has been used correctly.

\subsubsection{Tests for Visualization} \label{Tests for Visualization}

The below test will verify that the visualization produced during the inference phase is correct.

\paragraph{Visualization Testing}
\begin{enumerate}

  \item{test-viz-correct\\}
  
  Control: Automatic
            
  Initial State: Any components aside from the inputs that are needed for the visualization pipeline to run.
            
  Input: A set of pre-configured bounding boxes will be loaded alongside a predetermine LiDAR pointcloud. Additionally, a set of viewing positions and resolutions
  to view the produced pointclouds from.

  Output: The output pointcloud visualizations with bounding boxes should look identical to pre-generated verified visualizations of the same scene and bounding boxes. These
  pre-generated visualizations will be created in the future and carefully examined to ensure correctness before setting up the test.
  
  Test Case Derivation: This is really just using the method of manufactured solutions to verify that the visualization pipeline is working as expected without bugs. A correct
  and matching visualization here does not guarantee a correctly functioning visualization pipeline but it is a necessary condition.
            
  How test will be performed: This test will automatically be run through pytest with the LiDAR pointcloud, bounding boxes, and verified visualizations being generated beforehand.
  Matplotlib.testing.compare.compare\_images will be run to compare each generated visualization with the corresponding verified visualization.
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}
The subsections below will detail the tests that will ensure that \ProjectName{}meets the nonfunctional requirements.

\subsubsection{Tests for Accuracy}

The test below will ensure that \ProjectName{}is at least relatively as accurate as the BEVFusion baseline (\cite{liang2022bevfusion}).
		
\paragraph{Accuracy Testing}

\begin{enumerate}

\item{test-mAP\\}

Type: Manual
					
Initial State: The BEVFusion model will be loaded with the BEVFusion configuration currently present in the OpenPCDet repository (\cite{openpcdet2020}).
Additionally, \ProjectName{}will be loaded with some chosen configuration.
					
Input/Condition: A training and validation set from the nuScenes dataset (\cite{caesar2020nuscenes}).
					
Output/Result: After both models have been trained on the nuScenes training set (\cite{caesar2020nuscenes}), their mAP metrics on the validation set will be returned. The
proposed solution's validation mAP should not fall below the BEVFusion mAP by more than a threshold that will be determined with my Domain Expert.
					
How test will be performed: Since model training is time-intensive, this test will mainly be conducted whenever major changes to \ProjectName{}are made.
The threshold will only serve as a guideline and after each run of this test, a discussion will be done with my Domain Expert and Masters Supervisor to determine
if the result is satisfactory.

\end{enumerate}

While Waymo (\cite{sun2020scalability})could be tested for as well, it is a much larger dataset that would take quite long to train on. Moreover, evaluating this test for the nuScenes dataset (\cite{caesar2020nuscenes})
alone should provide almost as much insight as evaluating on both (since in existing research, better performing models tend to do better across both datasets).

\subsubsection{Tests for Understandability}

\paragraph{Understandability Testing}

Rather than use a fixed test for understandability, it will be tested closer to the end of the project through code walkthroughs. Specifically,
I will provide a demonstration of how to use the library to my Domain Expert and optionally Project and Masters supervisors (if they have time).
I will ask them the following questions afterwords to gauge how easy to understand the code is.
\begin{enumerate}
  \item How do you think the code works at a high level?
  \item If you had to describe what $x$ (to be determined after implementation) class of submodules does, how would you do so?
  \item If you had to learn how to use this code with nothing but the documentation present in the repository and the code itself, how long do you think it would take you to do so?
\end{enumerate}

Afterwords, I will test further test their understanding of the code by letting them play around with it and getting them to perform the following tasks with assistance as necessary.
\begin{enumerate}
  \item Train a model using $x$ (to be determined after implementation) configuration.
  \item Evaluate a \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pretrained model} on the nuScenes validation dataset (\cite{caesar2020nuscenes}).
  \item Modify the \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{configuration} for this pretrained model to change the post-processing prediction range.
\end{enumerate}

Through the code walkthrough and their subsequent interaction with it, I hope to get at least a rough idea of how understandable the code is.

\subsubsection{Tests for Installability}

\paragraph{Installability Testing}

Since I implemented the code, it would not really make sense to test the installability of \progname{} myself.
This is because I am familiar with the bugs that I faced when initially installing the code and thus would be able to 
troubleshoot on the fly where the user might not be able to. Rather, I plan to sit down with either my domain expert 
or Masters Supervisor (Dr. Matthew Giamou) and have them perform the following steps with minimal guidance:

\begin{enumerate}
  \item Clone the repository from github to get a fresh copy by running "git clone URL"
  \item Create a conda environment from the provided environment.yml file by running "conda env create -f environment.yml" from the environment subfolder
  \item Setup the repository using OpenPCDet's setup.py file by running "python setup.py develop" from the base directory
  \item Follow OpenPCDet's dataset setup guide for the NuScenes Dataset as explained \href{https://github.com/open-mmlab/OpenPCDet/blob/master/docs/GETTING_STARTED.md}{here}
  \begin{enumerate}
    \item Since the full dataset is too large, I would tell them to install the mini version instead
    \item If they do not want to have to create a NuScenes account to download the dataset, they can simply download the mini set by running "wget https://d36yt3mvayqw5m.cloudfront.net/public/v1.0/v1.0-mini.tgz"
    \item The pip install step in OpenPCDet's dataset setup guide should be ignored as it is already installed in the conda environment
    \item The multi-modal data infos should be generated rather than the lidar-only infos
  \end{enumerate}
  \item Install the \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{pretrained BEVFusion model} and place it in the pretrained\_models folder
  \item Install the \href{https://drive.google.com/file/d/1v74WCt4_5ubjO7PciA5T0xhQc9bz_jZu/view}{Pretrained NuImages Swin-Transformer} and place it in the pretrained\_models folder
  \item Evaluate this pretrained model on the nuScenes mini validation dataset (\cite{caesar2020nuscenes}) (I will add a guide for this in the Github's README file)
\end{enumerate}

If they are able to perform all these steps without errors (or with minimal errors that a researcher in the field would be able to troubleshoot),
then I would consider this test to have passed. Otherwise, the steps that fail would provide insight into how the installability of the software could be improved.

\subsubsection{Tests for Performance}

\paragraph{Performance Testing}

Since the amount of time inference and training runs take to execute are highly dependant on factors such as GPU, ram, hyperparameter, and background-processes.
As such, rather than try to compare runtimes between different methods or compare to existing papers (which run on much more powerful setups with 4 to 8 GPUs), I aim
to simply print the amount of time the model takes to for each training batch and each inference batch. With this information, I plan on having a discussion with my Masters' 
Supervisor (Dr. Matthew Giamou) to discuss if the results seem reasonable.

\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
    & R1 & R2 & R3 & R4 & NFR1 & NFR2 \\
  \hline
  Camera Image Input Testing             &X& & & & & \\ \hline
  LiDAR Pointcloud Input Testing         & &X& & & & \\ \hline
  Model Training Consistency Testing     & & &X& & & \\ \hline
  Visualization Testing                  & & & &X& & \\ \hline
  Accuracy Testing                       & & & & &X& \\ \hline
  Understandability Testing              & & & & & &X\\ \hline
  Installability Testing                 & & & & & &X\\ \hline
  Performance Testing                    & & & & & & \\ \hline
  \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between Requirements and Tests}
  \label{Table:R_trace}
  \end{table}

\newpage

\section{Unit Test Description}

This section focuses on testing procedures for individual self-contained modules defined in the 
\href{https://github.com/alaapgrandhi/equivariant-sensor-fusion/blob/main/docs/Design/SoftArchitecture/MG.pdf}{MG} and 
\href{https://github.com/alaapgrandhi/equivariant-sensor-fusion/blob/main/docs/Design/SoftDetailedDes/MIS.pdf}{MIS} documents. 
The idea behind the tests in this section is to verify that the modules are performing their functions as described in the module interface 
specification. Where possible, tests will be automated through PyTest and Github Actions, but a major proportion of tests must be conducted 
manually due to them requiring GPU compute or visual inspection of outputs.

\subsection{Unit Testing Scope}

For the purposes of these tests, several modules are not tested. The Logger \ref{MIS-ModuleLog}, Model \ref{MIS-ModuleModel}, Loss \ref{MIS-ModuleLoss}, Optimizer \ref{MIS-ModuleOptim}, and OpenPCDet Layers \ref{MIS-ModulePCDet} Modules are not tested here as they are directly inherited from OpenPCDet.
The PyTorch module \ref{MIS-ModuleTorch} is similarly not tested as it is a well-defined and well-tested library. The plotting module \ref{MIS-ModulePlot} is not tested in this section as it is already covered by \ref{Tests for Visualization}. Similarly, the data processing module \ref{MIS-ModuleDataProc} is not tested here as it is already covered by \ref{Input Format Testing}. 
The evaluation \ref{MIS-ModuleEval}, checkpoint \ref{MIS-ModuleCkpt}, config \ref{MIS-ModuleCfg}, and data \ref{MIS-ModuleData} modules are all evaluated together as they are mostly unchanged from OpenPCDet and can be verified through a single evaluation run. The remaining modules (training \ref{MIS-ModuleTrain}, inference \ref{MIS-ModuleInfer}, and equivariant layers \ref{MIS-ModuleEQ}) are all equally important as they directly contribute to the usefulness of the software.

\subsection{Tests for Functional Requirements}

\subsubsection{Training Module \ref{MIS-ModuleTrain}}

Since this module's purpose is to define a procedure by which the model is trained using the 
training dataloader, the model, the loss, and the optimizer, it is important to verify that the training loss
actually lowers as training proceeds. This is in alignment with functional requirement R3 from the SRS. 
Owing to the fact that this training loss is continuously logged during training, the test below is 
sufficient for determining if the loss does indeed decrease during training.

\begin{enumerate}

\item{test-train\\}

Type: Manual

Initial State: N/A
					
Input: A training run characterized by a pre-determined configuration (the exact configuration will be linked to once set in stone) will be run on the NuScenes dataset. Since the end-goal of this project is to use an equivariant network to improve fusion performance, the configuration used for this training run must contain equivariant layers (cannot simply use a pre-existing BEVFusion model).
					
Output: The graph of the training loss over time should show a generally decreasing curve (spikes up between neighbouring steps are okay, but the smoothed curve should show a decreasing loss for at least 10 epochs).

Test Case Derivation: If the model is effectively being trained to minimize the bounding box loss on the training set, then this value should generally decrease as the model trains more. Due to the randomness of batch-based deep-learning, it cannot be assumed that the training loss will monotically decrease without spikes up in-between.

How test will be performed: Since training is gpu-intensive, this test cannot be run automatically. Thus, a training run with the pre-determined configuration must be manually started. 
    
\end{enumerate}

\subsubsection{Inference Module \ref{MIS-ModuleInfer}}

Since this model effectively just relies on external modules to load the data and model, the key functionality that needs 
to be verified is the way it routes data to the model and then model outputs to the plotting module. This (in addition to whether it is calling other modules correctly) 
can be determined by the test below as it explicitly tests the full module end-to-end using the method of manufactured solutions. This aligns with functional requirement R4.

\begin{enumerate}

  \item{test-infer\\}
  
  Type: Automatic
            
  Initial State: A pre-determined multiview image and LiDAR pointcloud (these will be handpicked from their respective subfolders in NuScenes/v1.0-trainval/samples/ and put into the test's directory) will be chosen. Additionally, 
a `model' that simply outputs a box at the center (arithmetic mean) of the LiDAR pointcloud with standard deviation height, width, and length will be chosen.  
            
  Input: The inference module will be called with the data and model from above. 
            
  Output: The visualization output by the inference pipeline here should exactly match a pre-produced plot. This pre-produced plot will be created by 
computing the mean and standard deviations on the LiDAR pointcloud beforehand and passing the corresponding box alongside the pointcloud into the plotting module.  
  
  Test Case Derivation: Since this is unit testing, we can assume that the model and plotting module are working correctly here. Thus, this test will be explicitly verifying 
that the inference module is correctly routing the data to the model and then routing the model's output to the plotting module.  
  
  How test will be performed: This will automatically be run through pytest with the pre-produced plot being generated beforehand and placed in the test's directory. Matplotlib.testing.compare\_images will be run to compare the generated visualization with the pre-generated plot.
      
  \end{enumerate}

\subsubsection{Evaluation \ref{MIS-ModuleEval}, Checkpoint \ref{MIS-ModuleCkpt}, Config \ref{MIS-ModuleCfg} and Data \ref{MIS-ModuleData} Modules}

Normally, it would be more prudent to have separate unit tests for different modules. However, 
since the evaluation, config, checkpoint, and data (for training and evaluation) modules are inherited almost entirely from the 
base OpenPCDet repository, the tests for these can be done together. By using the pre-defined \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{BEVFusion configuration} and 
the corresponding \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{checkpoint file} from OpenPCDet, we can check if our mAP metrics match those reported on their github.
If so, then we know that the data (using the NuScenes eval loader), config (loading their config), checkpoint (loading their checkpoint), 
and evaluation (using the evaluator they define) modules are working as expected. This implicitly covers requirements R3, R4, and NFR1 due to the widespread use of these modules.

\begin{enumerate}

\item{test-eval\\}

Type: Manual
					
Initial State: The \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{BEVFusion config} and \href{https://drive.google.com/file/d/1X50b-8immqlqD8VPAUkSKI0Ls-4k37g9/view}{BEVFusion checkpoint} will be loaded and ready. The NuScenes dataset will also be fully set up.
					
Input/Condition: An evaluation run will be conducted using the above config and checkpoint as input. 
					
Output/Result: The mAP statistics should closely match those reported on the OpenPCDet repository. A little lee-way will be given due to the non-deterministic 
nature of \href{https://pytorch.org/docs/stable/notes/numerical_accuracy.html}{PyTorch floating point computation}. 

How test will be performed: Since evaluation is gpu-intensive, this test cannot be run automatically. Thus, an evaluation run must be manually started.

\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\subsubsection{Equivariant Layers Module \ref{MIS-ModuleEQ}}

I believe that testing the equivariant layers module is important for nonfunctional requirement 
NFR1. Even though the equivariant properties of these layers do not directly factor into the mAP formula, 
this project aims to see whether these properties can improve alignment and thus mAP for fusion-based object detection.
Therefore, it is important to verify that the equivariant equation (if $x_{2}=gx_{1}$, $y(x_{2})=\rho(g)y(x_{1})$) does indeed hold for these layers. 
The tests below explicitly check this by visualizing how the features extracted through these layers respond to 
expected transformations in input.

\begin{enumerate}

  \item{test-feat-consistency-image\\}
  
  Type: Manual
            
  Initial State: The inference pipeline fully set up with a pretrained equivariant model set up (after training). 

  Input/Condition: The model will be run on a multiview image along with several SO(2)-transformed versions of the same multiview image.

  Output/Result: The image-stream features (before fusion between the image and LiDAR streams) produced by the model for the SO(2)-transformed versions should look like SO(2)-transformed versions of the features generated for the original image. If this holds, it means that input transformations do map to output transformations in a structured manner.

  How test will be performed: Since finite convolutional kernel sizes can cause slight deviations from the equivariant principle, the feature maps will need to be compared visually by domain experts (myself and Dr. Matthew Giamou) to verify if the equivariance property is holding.

  \item{test-feat-consistency-lidar\\}
  
  Type: Manual
            
  Initial State: The inference pipeline fully set up with a pretrained equivariant model set up (after training).

  Input/Condition: The model will be run on a LiDAR pointcloud along with several SO(3)-transformed versions of the same multiview image.

  Output/Result: The LiDAR-stream features (before fusion between the image and LiDAR streams) produced by the model for the SO(3)-transformed versions should look like SO(3)-transformed versions of the features generated for the original pointcloud. If this holds, it means that input transformations do map to output transformations in a structured manner.

  How test will be performed: Since finite convolutional kernel sizes can cause slight deviations from the equivariant principle, the feature maps will need to be compared visually by domain experts (myself and Dr. Matthew Giamou) to verify if the equivariance property is holding.

  \item{test-transform-mapping-image\\}
  
  Type: Automatic
            
  Initial State: The inference pipeline fully set up with a pretrained equivariant model set up (after training).

  Input/Condition: The model will be run on a multiview image along with several SO(2)-transformed versions of the same multiview image.

  Output/Result: Since the layers in this case are determined by choosing an explicit mapping between input and output transformations, a set of mathematical equations can be solved to determine the inverse output transformation for any input transformation. Thus, applying the inverse output transformation to the features from the SO(2)-transformed multi-view images should produce feature maps that are nearly identical to the feature map of the untransformed multi-view image.

  How test will be performed: This test will automatically be run through pytest with the inverse output transformation being calculated beforehand. The untransformed multi-view image features will be compared to the inverse transformed features from the SO(2)-transformed multi-view images using Matplotlib.testing.compare\_images. An empirically determined similarity threshold will be used (10 average pixel difference to start with).
  
  \item{test-transform-mapping-lidar\\}
  
  Type: Automatic
            
  Initial State: The inference pipeline fully set up with a pretrained equivariant model set up (after training).

  Input/Condition: The model will be run on LiDAR pointcloud along with several SO(3)-transformed versions of the same pointcloud.

  Output/Result: Much like with the previous test, we should be able to derive a structured inverse output transformation for any input transformation. Then, applying this inverse to the features from the SO(3)-transformed pointclouds should produce feature maps that are nearly identical to the feature map of the untransformed pointcloud.

  How test will be performed: This test will automatically be run through pytest with the inverse output transformation being calculated before-hand. The untransformed features will be compared to the inverse transformed features using the formula $Err=\frac{||Feat_{un}-Feat_{inv}||}{||Feat_{un}||}$. From here, an acceptable error threshold will be empirically determined (5\% to start).

\end{enumerate}
  

\subsection{Traceability Between Test Cases and Modules}

\newpage
\begin{landscape}
\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    & \ref{MIS-ModuleTrain} & \ref{MIS-ModuleInfer} & \ref{MIS-ModuleEval} & \ref{MIS-ModuleLog} & \ref{MIS-ModuleModel} & \ref{MIS-ModuleLoss} & \ref{MIS-ModuleOptim} & \ref{MIS-ModulePlot} & \ref{MIS-ModuleCkpt} & \ref{MIS-ModuleData} & \ref{MIS-ModuleEQ} & \ref{MIS-ModulePCDet} & \ref{MIS-ModuleTorch} & \ref{MIS-ModuleCfg} & \ref{MIS-ModuleDataProc} \\
  \hline
  Camera Image Input Testing             & & & & & & & & & & & & & & &X\\ \hline
  LiDAR Pointcloud Input Testing         & & & & & & & & & & & & & & &X\\ \hline
  Model Training Consistency Testing     &X& &X&X&X&X&X& &X&X&X&X& & & \\ \hline
  Visualization Testing                  & & & & & & & &X& & & & & & & \\ \hline
  Accuracy Testing                       &X& &X&X&X&X&X& &X&X&X&X& & & \\ \hline
  Understandability Testing              & & & & & & & & & & & & & & & \\ \hline
  Installability Testing                 & & & & & & & & & & & & & & & \\ \hline
  Performance Testing                    & & & & & & & & & & & & & & & \\ \hline
  Training Module Testing                &X& & & & & & & & & & & & & & \\ \hline
  Inference Module Testing               & &X& & & & & & & & & & & & & \\ \hline
  Eval, Ckpt, Cfg, Data Module Testing   & & &X& & & & & &X&X& & & &X& \\ \hline
  Equivariant Layers Module Testing      & & & & & & & & & & &X& & & & \\ \hline
  \hline
  \end{tabular}
  \caption{Traceability Matrix Showing the Connections Between Requirements and Tests}
  \label{Table:R_trace}
  \end{table}
\end{landscape}

\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

N/A

\end{document}