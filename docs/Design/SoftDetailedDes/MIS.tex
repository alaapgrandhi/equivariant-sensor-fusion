\documentclass[12pt, titlepage]{article}

\usepackage{amsmath, mathtools}

\usepackage[round]{natbib}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{xr}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{xfrac}
\usepackage{tabularx}
\usepackage{float}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[section]{placeins}
\usepackage{caption}
\usepackage{fullpage}

\hypersetup{
bookmarks=true,     % show bookmarks bar?
colorlinks=true,       % false: boxed links; true: colored links
linkcolor=red,          % color of internal links (change box color with linkbordercolor)
citecolor=blue,      % color of links to bibliography
filecolor=magenta,  % color of file links
urlcolor=cyan          % color of external links
}

\usepackage{array}

\newcommand{\ProjectName}{ESF}

\externaldocument{../../SRS/SRS}

\input{../../Comments}
\input{../../Common}

\begin{document}

\title{Module Interface Specification for \ProjectName{}}

\author{Alaap Grandhi}

\date{\today}

\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Mar 21 & 1.0 & First Draft\\
Mar 22 & 1.1 & Added units/type\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

See SRS Documentation at \url{https://github.com/alaapgrandhi/equivariant-sensor-fusion/tree/main/docs/SRS}

\newpage

\tableofcontents

\newpage

\pagenumbering{arabic}

\section{Introduction}

The following document details the Module Interface Specifications for
\ProjectName: Equivariant Learning-based Camera-LiDAR 3D object detection in Autonomous Driving.

Complementary documents include the System Requirement Specifications
and Module Guide.  The full documentation and implementation can be
found at \url{https://github.com/alaapgrandhi/equivariant-sensor-fusion}.

\section{Notation}

The structure of the MIS for modules comes from \citet{HoffmanAndStrooper1995},
with the addition that template modules have been adapted from
\cite{GhezziEtAl2003}.  The mathematical notation comes from Chapter 3 of
\citet{HoffmanAndStrooper1995}.  For instance, the symbol := is used for a
multiple assignment statement and conditional rules follow the form $(c_1
\Rightarrow r_1 | c_2 \Rightarrow r_2 | ... | c_n \Rightarrow r_n )$.

The following table summarizes the primitive data types used by \ProjectName. 

\begin{center}
\renewcommand{\arraystretch}{1.2}
\noindent 
\begin{tabular}{l l p{7.5cm}} 
\toprule 
\textbf{Data Type} & \textbf{Notation} & \textbf{Description}\\ 
\midrule
character & char & a single symbol or digit\\
integer & $\mathbb{Z}$ & a number without a fractional component in (-$\infty$, $\infty$) \\
natural number & $\mathbb{N}$ & a number without a fractional component in [1, $\infty$) \\
real & $\mathbb{R}$ & any number in (-$\infty$, $\infty$)\\
\bottomrule
\end{tabular} 
\end{center}

\noindent
The specification of \ProjectName \ uses some derived data types: sequences, strings, and
tuples. Sequences are lists filled with elements of the same data type. Strings
are sequences of characters. Tuples contain a list of values, potentially of
different types. In addition, \ProjectName \ uses functions, which
are defined by the data types of their inputs and outputs. Local functions are
described by giving their type signature followed by their specification.

In addition to the above, str is used to denote a sequence of chars and * is used to denote the convolution operator.

\section{Module Decomposition}

The following table is taken directly from the Module Guide document for this project.

\begin{table}[h!]
  \centering
  \begin{tabular}{p{0.3\textwidth} p{0.6\textwidth}}
  \toprule
  \textbf{Level 1} & \textbf{Level 2}\\
  \midrule
  
  {Hardware-Hiding Module} & ~ \\
  \midrule
  
  \multirow{7}{0.3\textwidth}{Behaviour-Hiding Module} 
  & Config Module\\
  & Data Module\\
  & Model Module\\
  & Checkpoint Module\\
  & Training Module\\
  & Inference Module\\
  & Loss Module\\ 
  & Evaluation Module\\
  & Optimization Module\\
  & Data Processing Module\\
  & Equivariant Layer Module\\
  & OpenPCDet Layer Module\\
  \midrule
  
  \multirow{3}{0.3\textwidth}{Software Decision Module}
  & Plotting Module\\
  & PyTorch Module\\
  & Logging Module\\
  \bottomrule
  
  \end{tabular}
  \caption{Module Hierarchy}
  \label{TblMH}
  \end{table}
\newpage
~\newpage

\section{MIS of Training Module} \label{ModuleTrain} 

\subsection{Module}



\subsection{Uses}
\begin{enumerate}
  \item PyTorch Dataloader (\href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{PyTorch Documentation})
  \item PyTorch Optimizer (\href{https://pytorch.org/docs/stable/optim.html}{PyTorch Documentation})
  \item PyTorch Loss (\href{https://pytorch.org/docs/stable/nn.html#loss-functions}{PyTorch Documentation})
  \item Model (\ref{ModuleModel})
  \item Checkpoint (\ref{ModuleCkpt})
  \item Logger (\ref{ModuleLog})
\end{enumerate}

\subsection{Syntax}



\subsubsection{Exported Constants}

\begin{itemize}
  \item NUM\_EPOCHS: The number of epochs to train for.
\end{itemize}

\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
train & - & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}
\begin{itemize}
  \item evalLoader (PyTorch Dataloader)
  \item trainLoader (PyTorch Dataloader)
  \item optim (Optimizer Module)
  \item model (Model Module)
  \item loss (Loss Module)
\end{itemize}

\subsubsection{Environment Variables}


\subsubsection{Assumptions}


\subsubsection{Access Routine Semantics}


\noindent train():
\begin{itemize}
\item transition: Creates the evaluation dataloader, training dataloader, optimizer, model, and loss modules by calling their respective constructors (Data Module's constructor for the dataloaders). References to these are then stored in the corresponding state variables. Once this is done, trainEpoch and evalModel can be called in a loop for a set number of epochs (NUM\_EPOCHS).
\item output: N/A
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\begin{enumerate}
  \item \textbf{trainEpoch}(trainLoader (PyTorch Dataloader), optimizer (Optimizer Module), model (Model Module), loss (Loss Module)):
  \begin{enumerate}
    \item \textbf{Input:} The dataloader of training data, the model to be optimized, the ADAM optimizer, and the loss function for backpropagation.
    \item \textbf{Method Description:} This method optimizes the model over the input training data to lower the given loss that the model incurs. 
    Statistics describing the performance of the model (loss) over the training set are logged using the global logger.
    \item \textbf{Output:} N/A 
  \end{enumerate}
  \item \textbf{evalModel}(evalLoader (PyTorch Dataloader), model (Model Module)):
  \begin{enumerate}
    \item \textbf{Input:} The dataloader of evaluation data and the model to be evaluated.
    \item \textbf{Method Description:} This method collects the mAP metrics obtained from applying the current model to the evaluation dataset. These
    metrics are logged using the global logger. The current state of the model is additionally saved using the checkpointing system.
    \item \textbf{Output:} N/A
  \end{enumerate}
\end{enumerate}

\newpage

\section{MIS of Inference Module} \label{ModuleInfer} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item PyTorch Dataloader (\href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{PyTorch Documentation})
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
  \item Checkpoint (\ref{ModuleCkpt})
  \item Model (\ref{ModuleModel})
  \item Logger (\ref{ModuleLog})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
inference & imagePath (str), lidarPath (str), boundingBoxesGT (PyTorch Tensor $R^{g\times{}n_{b}}$), Model (Model Module), Checkpoint (Checkpoint Module) & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}


\subsubsection{Environment Variables}


\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent inference():
\begin{itemize}
\item transition: N/A 
\item output: N/A
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}
\begin{enumerate}
  \item \textbf{getProcessedData}(imagePath (str), lidarPath (str)) $\rightarrow$ imageTens (PyTorch Tensor $R^{n_{cams}\times{}3\times{}h\times{}w}$), lidarTens (Pytorch Tensor $R^{3\times{}n_{l}}$):
  \begin{enumerate}
    \item \textbf{Input:} The string paths to the image and lidar files for the scene to infer on.
    \item \textbf{Method Description:} This method essentially is just a local wrapper for the Data Processing Module's process access program.
    \item \textbf{Output:} The tensors for the multiview images and the LiDAR pointcloud, obtained using the Data Processing Module.
  \end{enumerate}
  \item \textbf{getPredBoundingBoxes}(imageTens (PyTorch Tensor $R^{n_{cams}\times{}3\times{}h\times{}w}$), lidarTens (PyTorch Tensor $R^{n_{l}\times{}3}$), Model (Model Module)) $\rightarrow$ boundingBoxesPred (PyTorch Tensor $R^{g\times{}n_{\hat{b}}}$):
  \begin{enumerate}
    \item \textbf{Input:} The model to be used for inference in addition to the input data (multiview camera images and a LiDAR pointcloud).
    \item \textbf{Method Description:} This method uses the model to get a set of predicted bounding boxes for the given input data.
    \item \textbf{Output:} The predicted bounding boxes.
  \end{enumerate}
  \item \textbf{plotBoundingBoxes}(lidarTens (PyTorch Tensor $R^{n_{l}\times{}3}$), boundingBoxesGT (PyTorch Tensor $R^{g\times{}n_{b}}$), boundingBoxesPred (PyTorch Tensor $R^{g\times{}n_{\hat{b}}}$)):
  \begin{enumerate}
    \item \textbf{Input:} The input LiDAR pointcloud alongside the ground truth and predicted bounding boxes for this scene. 
    \item \textbf{Method Description:} This method displays a 3D plot of the ground truth and predicted bounding boxes on the input LiDAR pointcloud to allow for visual inspection (using the plotting module).
    \item \textbf{Output:} N/A
  \end{enumerate}
\end{enumerate}

\newpage

\section{MIS of Evaluation Module} \label{ModuleEval} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item PyTorch Dataloader (\href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{PyTorch Documentation})
  \item Model (\ref{ModuleModel})
  \item Checkpoint (\ref{ModuleCkpt})
\end{itemize}


\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
  \begin{tabular}{p{2cm}|p{6cm}|p{2cm}|p{2cm}}
  \hline
  \textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
  \hline
  eval & evalLoader (PyTorch Dataloader), model (Model Module), ckpt (Checkpoint Module) & metrics (Named Tuple of (str, $R$)) & - \\
  \hline
  \end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}


\subsubsection{Environment Variables}


\subsubsection{Assumptions}

\subsubsection{Access Routine Semantics}
\noindent eval():
\begin{itemize}
\item transition: N/A 
\item output: This function will run the input model over the evaluation loader and collect the mAP statistics over this set of data. These statistics will then be returned as output. If a checkpoint is passed in, the model will first load the given checkpoint. Details of the mAP calculation can be seen in the SRS's IM4.
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}


\newpage

\section{MIS of Logger Module} \label{ModuleLog} 
This module is simply used to represent a wrapper around a global Tensorboard logger (\href{https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html}{PyTorch tutorial on using Tensorboard})
that can be used in any module to log metrics to a logging file. It interacts with the file system environment variable and can be used to visualize logs after saving them.
\newpage

\section{MIS of Model Module} \label{ModuleModel} 

\subsection{Module}

\subsection{Uses}
\begin{itemize}
  \item Is a subclass of PyTorch Module (\href{https://pytorch.org/docs/stable/generated/torch.nn.Module.html}{PyTorch Documentation})
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
  \item Equivariant Layers (\ref{ModuleEQ})
  \item OpenPCDet Layers (\ref{ModulePCDet})
  \item Configuration (\ref{ModuleCfg})
\end{itemize}

\subsection{Syntax}

\subsubsection{Exported Constants}

\subsubsection{Exported Access Programs}

\begin{center}
  \begin{tabular}{p{2cm}|p{4cm}|p{4cm}|p{2cm}}
  \hline
  \textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
  \hline
  init & - & - & - \\
  \hline
  forward & imgTens (PyTorch Tensor $R^{n_{cams}\times{}3\times{}h\times{}w}$), lidarTens (PyTorch Tensor $R^{n_{l}\times{}3}$) & boundingBoxesPred (PyTorch Tensor $R^{g\times{}n_{\hat{b}}}$) & - \\
  \hline
  \end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}
\begin{itemize}
  \item pcdetLayers (OpenPCDet Layers Module)
  \item eqLayers (Equivariant Layers Module)
\end{itemize}

\subsubsection{Environment Variables}


\subsubsection{Assumptions}

\subsubsection{Access Routine Semantics}
\noindent init():
\begin{itemize}
\item transition: This function will load the model-related configuration from the global configuration module and will then create the OpenPCDet and Equivariant Layers accordingly. References to these layers will be stored in the pcdetLayers and eqLayers state variables. 
\item output: N/A
\item exception: N/A
\end{itemize}

\noindent forward():
\begin{itemize}
\item transition: N/A 
\item output: This function will run the input data (LiDAR and Images) through the OpenPCDet and Equivariant layers to produce a set of predicted bounding boxes. These predicted bounding boxes will then be returned. 
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}


\newpage

\section{MIS of Loss Module} \label{ModuleLoss} 

\subsection{Module}



\subsection{Uses}

\begin{itemize}
  \item Is a subclass of PyTorch Loss (\href{https://pytorch.org/docs/stable/nn.html#loss-functions}{PyTorch Documentation})
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
  \item Configuration (\ref{ModuleCfg})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{4cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
init & - & - & - \\
\hline
forward & boundingBoxesPred (PyTorch Tensor $R^{g\times{}n_{\hat{b}}}$), boundingBoxesGT (PyTorch Tensor $R^{g\times{}n_{b}}$) & Loss Value (PyTorch Tensor $R$) & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

\begin{itemize}
  \item hyperparameters (Sequence of $R$)
\end{itemize}

\subsubsection{Environment Variables}

\subsubsection{Assumptions}

\subsubsection{Access Routine Semantics}
\noindent init():
\begin{itemize}
\item transition: This method will set the loss module's hyperparamters according to the loss-related configuration from the global configuration module. 
\item output: N/A
\item exception: N/A
\end{itemize}

\noindent forward():
\begin{itemize}
\item transition: N/A 
\item output: This method will use the predicted and ground truth bounding boxes to regress and output a loss value. Details of the loss function to be implemented can be seen in the SRS's IM2.
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of Optimizer Module} \label{ModuleOptim} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item Is a wrapper around the PyTorch ADAM Optimizer (\href{https://pytorch.org/docs/stable/optim.html}{PyTorch Documentation})
  \item PyTorch Parameter (\href{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html}{PyTorch Documentation})
  \item Configuration (\ref{ModuleCfg})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
init & modelParams (Sequence of PyTorch Parameter) & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

\begin{itemize}
  \item hyperparameters (Sequence of $R$)
  \item modelParameters (Sequence of PyTorch Parameter)
\end{itemize}

\subsubsection{Environment Variables}



\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent init():
\begin{itemize}
\item transition: This method will set the loss module's hyperparameters according to the optimizer-related configuration from the global configuration module. It will also save a reference to passed in model parameters in the modelParameters state variable. Details of the ADAM Optimizer can be seen in the SRS's IM3.
\item output: N/A
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of Plotting Module} \label{ModulePlot} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item Open3D Oriented Bounding Boxes (\href{https://www.open3d.org/docs/latest/python_api/open3d.geometry.OrientedBoundingBox.html}{Open3D Documentation})
  \item Open3D Pointclouds (\href{https://www.open3d.org/docs/release/python_api/open3d.geometry.PointCloud.html}{Open3D Documentation})
  \item Open3D in general (\href{https://www.open3d.org/docs/release/introduction.html}{Open3D Documentation})
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
plot & lidarTens (PyTorch Tensor $R^{n_{l}\times{}3}$), boundingBoxesGT (PyTorch Tensor $R^{g\times{}n_{b}}$), boundingBoxesPred (PyTorch Tensor $R^{g\times{}n_{\hat{b}}}$) & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

\subsubsection{Environment Variables}

\begin{itemize}
  \item Computer Screen
\end{itemize}

\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent plot():
\begin{itemize}
\item transition: The LiDAR pointcloud is first converted into an Open3D Pointcloud. Then, the bounding boxes are converted into the Open3d OrientedBoundingBox type. Finally, these are all visualized together in a single 3D plot using Open3D's draw\_geometries function. This outputs a plot and thus updates the computer screen environment variable.
\item output: N/A
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of Checkpoint Module} \label{ModuleCkpt} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item PyTorch Module (\href{https://pytorch.org/docs/stable/generated/torch.nn.Module.html}{PyTorch Documentation})
  \item PyTorch State Dictionary (\href{https://pytorch.org/tutorials/beginner/saving_loading_models.html}{PyTorch Documentation})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{3cm}|p{5cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
saveCheckpoint & model (PyTorch Module), CkptPath (str) & - & - \\
\hline
loadCheckpoint & model (PyTorch Module), CkptPath (str) & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}



\subsubsection{Environment Variables}

\begin{itemize}
  \item File System
\end{itemize}

\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent saveCheckpoint():
\begin{itemize}
\item transition: The input model's state dictionary is saved to a pytorch tensor file at the input path. 
\item output: N/A
\item exception: N/A
\end{itemize}


\noindent loadCheckpoint():
\begin{itemize}
\item transition: The input model's parameters are set using the state dictionary saved at the input path.
\item output: N/A
\item exception: N/A
\end{itemize}


\subsubsection{Local Functions}

\newpage

\section{MIS of Data Module} \label{ModuleData} 

\subsection{Module}



\subsection{Uses}
\begin{enumerate}
  \item PyTorch Dataset and Dataloader (\href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{PyTorch Documentation})
  \item Configuration (\ref{ModuleCfg})
\end{enumerate}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{3cm}|p{5cm}|p{4cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
init & isEval (boolean) & - & - \\
\hline
getDataloader & - & loader (PyTorch Dataloader) & -  \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}
\begin{itemize}
  \item dataset (PyTorch Dataset)
\end{itemize}

\subsubsection{Environment Variables}
\begin{enumerate}
  \item File System
\end{enumerate}

\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent init():
\begin{itemize}
\item transition: This function will load the data-related configuration from the global configuration module and will then create the dataset accordingly.
This dataset will either be the NuScenes (\cite{caesar2020nuscenes}) or the Waymo (\cite{sun2020scalability}) dataset for now and will be stored in the dataset state variable. If isEval is false,
the training portion of the dataset will be loaded. If isEval is true, the evaluation portion of the dataset will be loaded.
\item output: N/A
\item exception: N/A
\end{itemize}

\noindent getDataloader():
\begin{itemize}
  \item transition: N/A
  \item output: This function will use the dataset state variable to create a PyTorch Dataloader which will then be returned.
  \item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of Equivariant Layers Module} \label{ModuleEQ} 

This module is meant to represent the equivariant layers I will be adding into the OpenPCDet repository for the novel part
of my project. To provide context for the reader, equivariant layers
refer to neural network layers that preserve input transformations when generating output features. For example, a rotated image of a cat 
will result in similarly rotated output features (when compared to features generated from an upright image of a cat). This work by Cohen et Al. (\cite{cohen2016group})
more clearly explains this concept.


This module will encapsulate two different types of equivariant layers. The first of these is steerable kernel 
equivariant CNNs (\cite{cohen2016steerable}) extended to include both 2D and 3D symmetries using the escnn repository (\cite{cesa2022a}). 
The second of these is equivariant transformer networks (\cite{tai2019equivariant}). Existing OpenPCDet layers can essentially then be directly
swapped out for these equivariant alternatives. 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item PyTorch Parameter (\href{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html}{PyTorch Documentation})
  \item Steerable CNNs (\cite{cesa2022a})
  \item Equivariant Transformers (\cite{tai2019equivariant})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{4cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
init & - & - & - \\
\hline
forward & - & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}
\begin{itemize}
  \item layerParameters (Sequence of PyTorch Parameter)
\end{itemize}

\subsubsection{Environment Variables}



\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent init():
\begin{itemize}
\item transition: The parameters for the layer are randomly initialized according to a uniform distribution around 0 (saved in the layerParameters state variable). 
\item output: N/A
\item exception: N/A
\end{itemize}


\noindent forward():
\begin{itemize}
\item output:
\begin{itemize}
  \item \textbf{For the CNN case:} Since the CNN case is simpler, the math relating to it can be shown here itself. The result $y$ of this formula is returned:
  \newline\newline
  $y[m,n](k,x)=\sum_{i=-w}^{w}\sum_{j=-h}^{h}k[w+i,h+j]x[m+i,n+j]\rightarrow{}y(k,x)=k*x$ \newline
  such that if $x_{2}=gx_{1}$ for some $g$ in the group of considered transformations, $y(x_{2})=\rho(g)y(x_{1})$
  for a single function $\rho$.
  \newline
  \item \textbf{For the Transformer case:} The math here is more complicated and so I will simply refer the reader to the paper by Tai et. al (\cite{tai2019equivariant}). The output of this layer is returned. Generally though for attention function f,
  the following can be written similar to the CNN case:
  $x_{2}=gx_{1} \rightarrow{} y(k,x_{2})=\rho(g)y(k,x_{1})$ 
\end{itemize}
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of OpenPCDet Layers Module} \label{ModulePCDet} 
This module is simply used to represent the existing layers in the OpenPCDet library (\cite{openpcdet2020}).
Rather than highlight specific relevant layer types and modules, I would encourage the reader to read through the BEVFusion (\cite{liang2022bevfusion})
configuration file (\href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{BEVFusion Config File}) and 
associated code sections (\href{https://github.com/open-mmlab/OpenPCDet/tree/master/pcdet/models}{Model-Related OpenPCDet Code}).

\newpage

\section{MIS of PyTorch Module} \label{ModuleTorch} 
This module is simply used to represent the PyTorch library referenced in the other modules.
The main list of components that are used from this library is as follows:
\begin{itemize}
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
  \item PyTorch Module (\href{https://pytorch.org/docs/stable/generated/torch.nn.Module.html}{PyTorch Documentation})
  \item PyTorch Dataloader (\href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{PyTorch Documentation})
  \item PyTorch Optimizer (\href{https://pytorch.org/docs/stable/optim.html}{PyTorch Documentation})
  \item PyTorch Loss (\href{https://pytorch.org/docs/stable/nn.html#loss-functions}{PyTorch Documentation})
  \item PyTorch State Dictionary (\href{https://pytorch.org/tutorials/beginner/saving_loading_models.html}{PyTorch Documentation})
  \item PyTorch Parameter (\href{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html}{PyTorch Documentation})
\end{itemize}

\newpage

\section{MIS of Config Module} \label{ModuleCfg} 

\subsection{Module}



\subsection{Uses}


\subsection{Syntax}



\subsubsection{Exported Constants}
\begin{itemize}
  \item CONFIG\_PATH
\end{itemize}


\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{3cm}|p{5cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
init & - & - & - \\
\hline
getModelConfig & - & - & - \\
\hline
getDataConfig & - & - & - \\
\hline
getOptimConfig & - & - & - \\
\hline
getLossConfig & - & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

\begin{itemize}
  \item modelConfig (Sequence of $R$)
  \item dataConfig (Sequence of $R$)
  \item optimConfig (Sequence of $R$)
  \item lossConfig (Sequence of $R$)
\end{itemize}

\subsubsection{Environment Variables}

\begin{itemize}
  \item File System
\end{itemize}

\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent init():
\begin{itemize}
  \item transition: This method loads the yaml configuration file stored at the predefined constant path CONFIG\_PATH. Then, the dictionary loaded from this JSON file is split into model, data, optimizer, and loss sections. Each of these is saved to the corresponding state variable for this module.
  \item output: N/A
  \item exception: N/A
\end{itemize}

\noindent getModelConfig():
\begin{itemize}
\item transition: N/A 
\item output: This method returns the configuration parameters for the model module.
\item exception: N/A
\end{itemize}

\noindent getDataConfig():
\begin{itemize}
  \item transition: N/A
  \item output: This method returns the configuration parameters for the data module.
  \item exception: N/A
\end{itemize}

\noindent getOptimConfig():
\begin{itemize}
  \item transition: N/A
  \item output: This method returns the configuration parameters for the optimizer module.
  \item exception: N/A
\end{itemize}

\noindent getLossConfig():
\begin{itemize}
  \item transition: N/A
  \item output: This method returns the configuration parameters for the loss module.
  \item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of Data Processing Module} \label{ModuleDataProc} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{5cm}|p{3cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
process & imgPath (str), lidarPath (str) & imgTens (PyTorch Tensor $R^{n_{cams}\times{}3\times{}h\times{}w}$), lidarTens (PyTorch Tensor $R^{n_{l}\times{}3}$) & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}



\subsubsection{Environment Variables}



\subsubsection{Assumptions}

\newpage



\subsubsection{Access Routine Semantics}
\noindent process():
\begin{itemize}
\item transition: N/A 
\item output: Loads the multiview images and LiDAR pointcloud from their respective files and converts them into a standard PyTorch Tensor format that can be used by the model. The Tensor form of the multiview images and the LiDAR pointcloud are returned.
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}


\newpage

\bibliographystyle {plainnat}
\bibliography {../../../refs/References}

\newpage




\section{Appendix} \label{Appendix}

May be added in the future.


\end{document}