\documentclass[12pt, titlepage]{article}

\usepackage{amsmath, mathtools}

\usepackage[round]{natbib}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{xr}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{xfrac}
\usepackage{tabularx}
\usepackage{float}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[section]{placeins}
\usepackage{caption}
\usepackage{fullpage}

\hypersetup{
bookmarks=true,     % show bookmarks bar?
colorlinks=true,       % false: boxed links; true: colored links
linkcolor=red,          % color of internal links (change box color with linkbordercolor)
citecolor=blue,      % color of links to bibliography
filecolor=magenta,  % color of file links
urlcolor=cyan          % color of external links
}

\usepackage{array}

\newcommand{\ProjectName}{ESF}

\externaldocument{../../SRS/SRS}

\input{../../Comments}
\input{../../Common}

\begin{document}

\title{Module Interface Specification for \ProjectName{}}

\author{Alaap Grandhi}

\date{\today}

\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Mar 21 & 1.0 & First Draft\\
Mar 22 & 1.1 & Added units/type\\
Apr 17 & 2.0 & Incorporated Professor Smith's feedback \\
Apr 17 & 2.1 & Added Domain Expert feedback\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

See \href{https://github.com/alaapgrandhi/equivariant-sensor-fusion/tree/main/docs/SRS}{SRS Documentation} 

\subsection{Table of Symbols}

The following table of symbols is copied over from the SRS document to clarify symbols used in this document. Not all symbols in here show up in this document but they are put here nonetheless for consistency with the SRS.

\renewcommand{\arraystretch}{1.2}
%\noindent \begin{tabularx}{1.0\textwidth}{l l X}
\noindent \begin{longtable*}{l l p{12cm}} \toprule
\textbf{symbol} & \textbf{unit} & \textbf{description}\\
\midrule 
$f_x$ & $\mathbb{R}_{\geq0}$ pixels & The focal length of the given camera in the $x$ direction.
\\
$f_y$ & $\mathbb{R}_{\geq0}$ pixels & The focal length of the given camera in the $y$ direction.
\\
$c_x$ & $\mathbb{R}$ pixels & The principal point of the given camera in the $x$ direction.
\\
$c_y$ & $\mathbb{R}$ pixels & The principal point of the given camera in the $y$ direction.
\\
$s$ & $\mathbb{R}$ pixels & The skew of the given camera representing how far from perpendicular the $x$ and $y$ directions are.
\\
$\mathbf{K}$ & $\mathbb{R}^{3\times3}$ pixels & The intrinsic matrix of the given camera representing how 3D points are projected onto the 2D image plane.
\\
$n_p$ & $\mathbb{R}_{\geq0}$ & The number of pixels in a given input image.
\\
$\mathbf{P_i}$ & $\mathbb{R}^{2\times{}n_p}$ pixels & The set of $x$,$y$ coordinates for each of the $n_p$ pixels in a given image.
\\
$\mathbf{P_c}$ & $\mathbb{R}^{3\times{}n_p}$ m & The set of back-projected $x$,$y$,$z$ coordinates for each of the $n_p$ pixels in a given image. This is just $X_{c}$, $Y_{c}$, and $Z_{c}$ concatenated.
\\
$\mathbf{X_c}$ & $\mathbb{R}^{n_p}$ m & The set of back-projected $x$ coordinates for each of the $n_p$ pixels in a given image.
\\
$\mathbf{Y_c}$ & $\mathbb{R}^{n_p}$ m & The set of back-projected $y$ coordinates for each of the $n_p$ pixels in a given image.
\\
$\mathbf{Z_c}$ & $\mathbb{R}^{n_p}$ m & The set of back-projected $z$ coordinates for each of the $n_p$ pixels in a given image.
\\
$D$ & $\mathbb{R}^{3}\rightarrow{}\mathbb{R}^{3}$ & The distortion model of the given camera (accounts for lens effects).
\\
$\mathbf{P_w}$ & $\mathbb{R}^{3\times{}n_p}$ m & The set of back-projected, world reference frame $x$,$y$,$z$ coordinates for each of the $n_p$ pixels in a given image.
\\
$\mathbf{T_{c,w}}$ & $\mathbb{R}^{4\times{}4}$ & The extrinsics matrix that maps $x$,$y$,$z$ points from the given camera's frame of reference to the world reference frame.
\\ 
$\mathbf{R_{c,w}}$ & $\mathbb{R}^{3\times3}$ & The rotation matrix between the given camera's frame of reference and the world reference frame.
\\
$\mathbf{t_{c,w}}$ & $\mathbb{R}^{3}$ & The translation vector between the given camera's frame of reference and the world reference frame.
\\
$\mathbb{P}$ & $\mathbb{R}^{n_c}$ & A predicted probability distribution over the set of $n_c$ discrete classes.
\\
$\alpha_{gd}$ & $\mathbb{R}$ & A hyperparameter for gradient descent that controls how large each gradient update is.
\\
$L$ & $\mathbb{R}^{n_\theta}\rightarrow\mathbb{R}$ & A loss function that maps the parameters of a model to a single estimated loss value.
\\
$n_{\theta}$ & $\mathbb{R}_{\geq0}$ & The number of learnable parameters for a given model.
\\
$\mathbf{\theta}$ & $\mathbb{R}^{n_\theta}$ & The learnable parameters for a given model, where the number of parameters is $n_\theta$.
\\
$\mathit{TP}$ & $\mathbb{N}_{0}$ & The number of true positives for a given classification task. That is, the number of positive predictions that corresponded to positive ground truth.
\\
$\mathit{FP}$ & $\mathbb{N}_{0}$ & The number of false positives for a given classification task. That is, the number of positive predictions that corresponded to a negative ground truth.
\\
$\mathit{FN}$ & $\mathbb{N}_{0}$ & The number of false negatives for a given classification task. That is, the number of positive ground truths that were not predicted to be positive.
\\
$\mathbf{d_{c}}$ & $\mathbb{R}^{n_p}$ m & The estimated depth values for each of the pixels in a given image.
\\
$g$ & $\mathbb{R}_{\geq0}$ & The number of scalar values/parameters used to define a bounding box (dataset-dependent). For the nuScenes dataset, this is 10 corresponding to the 3D center position, height, width, length, and a quaternion angle representation. 
\\
$n_{b}$ & $\mathbb{R}_{\geq0}$ & The number of ground truth bounding boxes. 
\\
$n_{\hat{b}}$ & $\mathbb{R}_{\geq0}$ & The number of predicted bounding boxes. 
\\
$n_{c}$ & $\mathbb{R}_{\geq0}$ & The number of object classes (typically 3 corresponding to cars, pedestrians, and cyclists). 
\\
$\mathbf{B}$ & $\mathbb{R}^{g\times{}n_b}$ & The set of ground-truth bounding box attributes (i.e. center, height, width, etc.), where there are $n_b$ ground truth bounding boxes.
\\
$\mathbf{\hat{B}}$ & $\mathbb{R}^{g\times{}n_{\hat{b}}}$ & The set of predicted bounding box attributes, where there are $n_{\hat{b}}$ predicted bounding boxes.
\\
$\mathbf{\hat{C}_{\text{dist}}}$ & $\mathbb{R}^{n_c\times{}n_{\hat{b}}}$ & The set of predicted bounding box class probability distributions.
\\
$\mathbf{\hat{c}}$ & $\mathbb{N}_{0}^{n_{\hat{b}}}$ & The set of predicted bounding box classes.
\\
$\mathbf{c}$ & $\mathbb{N}_{0}^{n_b}$ & The set of ground truth bounding box classes.
\\
$\alpha_{fl}$ & $\mathbb{R}$ & A hyperparameter for controlling the influence of the focal loss term.
\\
$\gamma$ & $\mathbb{R}$ & A hyperparameter for controlling the effect of easy examples on the focal loss. In practice, this is class specific and is set to the inverse class frequency.
\\
$\beta_1$ & $\mathbb{R}$ & A hyperparameter for changing the inertia of the first moment in ADAM.
\\
$\beta_2$ & $\mathbb{R}$ & A hyperparameter for changing the inertia of the second moment in ADAM.
\\
$\tau_{IoU}$ & $\mathbb{R}$ & An IoU threshold that is used to determine whether a predicted and a ground truth bounding box sufficiently overlap to be considered paired.
\\
$h$ & $\mathbb{R}_{\geq0}$ & The height of a given input image. 
\\
$w$ & $\mathbb{R}_{\geq0}$ & The width of a given input image. 
\\
$n_{l}$ & $\mathbb{R}_{\geq0}$ & The number of points in an input lidar pointcloud. 
\\
$\mathcal{I}$ & $\mathbb{R}^{h\times{}w}$ & An input image with height $h$ and width $w$.
\\
$\mathcal{P}$ & $\mathbb{R}^{3\times{}n_l}$ & An input lidar pointcloud with $n_l$ points.
\\
$n_{cams}$ & $\mathbb{R}$ & The number of cameras on the autonomous vehicle (and thus the number of images in each multi-view image).
\\
\bottomrule
\end{longtable*}

\newpage

\tableofcontents

\newpage

\pagenumbering{arabic}

\section{Introduction}

The following document details the Module Interface Specifications for
\ProjectName: Equivariant Learning-based Camera-LiDAR 3D object detection in Autonomous Driving.

Complementary documents include the System Requirement Specifications
and Module Guide.  The full documentation and implementation can be
found at \url{https://github.com/alaapgrandhi/equivariant-sensor-fusion}.

\section{Notation}

The structure of the MIS for modules comes from \citet{HoffmanAndStrooper1995},
with the addition that template modules have been adapted from
\cite{GhezziEtAl2003}.  The mathematical notation comes from Chapter 3 of
\citet{HoffmanAndStrooper1995}.  For instance, the symbol := is used for a
multiple assignment statement and conditional rules follow the form $(c_1
\Rightarrow r_1 | c_2 \Rightarrow r_2 | ... | c_n \Rightarrow r_n )$.

The following table summarizes the primitive data types used by \ProjectName. 

\begin{center}
\renewcommand{\arraystretch}{1.2}
\noindent 
\begin{tabular}{l l p{7.5cm}} 
\toprule 
\textbf{Data Type} & \textbf{Notation} & \textbf{Description}\\ 
\midrule
character & char & a single symbol or digit\\
integer & $\mathbb{Z}$ & a number without a fractional component in (-$\infty$, $\infty$) \\
natural number & $\mathbb{N}$ & a number without a fractional component in [1, $\infty$) \\
real & $\mathbb{R}$ & any number in (-$\infty$, $\infty$)\\
\bottomrule
\end{tabular} 
\end{center}

\noindent
The specification of \ProjectName \ uses some derived data types: sequences, strings, and
tuples. Sequences are lists filled with elements of the same data type. Strings
are sequences of characters. Tuples contain a list of values, potentially of
different types. In addition, \ProjectName \ uses functions, which
are defined by the data types of their inputs and outputs. Local functions are
described by giving their type signature followed by their specification.

In addition to the above, str is used to denote a sequence of chars and * is used to denote the convolution operator.

\section{Module Decomposition}

The following table is taken directly from the Module Guide document for this project.

\begin{table}[h!]
  \centering
  \begin{tabular}{p{0.3\textwidth} p{0.6\textwidth}}
  \toprule
  \textbf{Level 1} & \textbf{Level 2}\\
  \midrule
  
  {Hardware-Hiding Module} & ~ \\
  \midrule
  
  \multirow{7}{0.3\textwidth}{Behaviour-Hiding Module} 
  & Config Module\\
  & Data Module\\
  & Model Module\\
  & Checkpoint Module\\
  & Training Module\\
  & Inference Module\\
  & Loss Module\\ 
  & Evaluation Module\\
  & Optimization Module\\
  & Data Processing Module\\
  & Equivariant Layer Module\\
  & OpenPCDet Layer Module\\
  \midrule
  
  \multirow{3}{0.3\textwidth}{Software Decision Module}
  & Plotting Module\\
  & PyTorch Module\\
  & Logging Module\\
  \bottomrule
  
  \end{tabular}
  \caption{Module Hierarchy}
  \label{TblMH}
  \end{table}
\newpage
~\newpage

\section{MIS of Training Module} \label{ModuleTrain}

\subsection{Module}



\subsection{Uses}
\begin{enumerate}
  \item PyTorch Dataloader (\href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{PyTorch Documentation})
  \item PyTorch Optimizer (\href{https://pytorch.org/docs/stable/optim.html}{PyTorch Documentation})
  \item PyTorch Loss (\href{https://pytorch.org/docs/stable/nn.html#loss-functions}{PyTorch Documentation})
  \item Model (\ref{ModuleModel})
  \item Checkpoint (\ref{ModuleCkpt})
  \item Logger (\ref{ModuleLog})
\end{enumerate}

\subsection{Syntax}



\subsubsection{Exported Constants}

\begin{itemize}
  \item NUM\_EPOCHS ($R$): The number of epochs to train for. For now, this constant equals 30, but this may be refined in future versions.
\end{itemize}

\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
setup & - & - & - \\
\hline
train & - & - & - \\
\hline
eval & - & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}
\begin{itemize}
  \item evalLoader (PyTorch Dataloader)
  \item trainLoader (PyTorch Dataloader)
  \item optim (Optimizer ADT)
  \item model (Model ADT)
  \item loss (Loss ADT)
\end{itemize}

\subsubsection{Environment Variables}


\subsubsection{Assumptions}


\subsubsection{Access Routine Semantics}

\noindent setup():
\begin{itemize}
\item transition: Creates the optim, model, and loss abstract objects by calling their respective constructors (For the Optimizer \ref{ModuleOptim}, Model \ref{ModuleModel}, and Loss \ref{ModuleLoss} Modules respectively). The trainLoader abstract object is created by calling the Data module's \ref{ModuleData} constructor with isEval set to False and then calling the getDataloader access program. The same process is followed for the evalLoader, but with isEval set to True. References to these are then stored in the corresponding state variables.
\item output: N/A
\item exception: N/A
\end{itemize}

\noindent train():
\begin{itemize}
\item transition: The model is trained on the trainLoader's data for NUM\_EPOCHS epochs using the ADAM optimizer described \href{https://arxiv.org/abs/1412.6980}{here}. In this process, the evalLoader, trainLoader, and the loss are simply accessed, while the optim and model objects are modified according to the training procedure.
\item output: The loss values for each batch are logged to the filesystem using the global Logger \ref{ModuleLog}.
\item exception: N/A
\end{itemize}

\noindent eval():
\begin{itemize}
  \item transition: N/A
  \item output: The mAP metrics for the current model are collected using the Evaluation Module's \ref{ModuleEval} eval access program. Only the evalLoader and model are passed in here since no checkpoint needs to be loaded. These metrics are logged to the filesystem using the global Logger \ref{ModuleLog}. Additionally, a checkpoint of the current model is saved to the file system using the Checkpoint Module's \ref{ModuleCkpt} saveCheckpoint exported access program (the CkptPath is up to the code writer's discretion as it is based on their local file setup).
\end{itemize}

\newpage

\section{MIS of Inference Module} \label{ModuleInfer} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item PyTorch Dataloader (\href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{PyTorch Documentation})
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
  \item Checkpoint (\ref{ModuleCkpt})
  \item Model (\ref{ModuleModel})
  \item Logger (\ref{ModuleLog})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
setupData & imagePath (str), lidarPath (str) & - & - \\
\hline
makePred & ckptPath (str) & - & - \\
\hline
inference & boundingBoxesGT (PyTorch Tensor $R^{g\times{}n_{b}}$) & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}
\begin{itemize}
  \item imageTens (PyTorch Tensor $R^{n_{cams}\times{}3\times{}h\times{}w}$)
  \item lidarTens (Pytorch Tensor $R^{3\times{}n_{l}}$)
  \item boundingBoxesPred (PyTorch Tensor $R^{g\times{}n_{\hat{b}}}$)
  \item model (Model ADT)
\end{itemize}

\subsubsection{Environment Variables}


\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent setupData(imagePath (str), lidarPath (str)):
\begin{itemize}
\item transition: This exported access program uses the Data Processing Module's \ref{ModuleDataProc} process access program to load the data and save it to the imageTens and lidarTens state variables.
\item output: N/A
\item exception: N/A
\end{itemize}

\noindent makePred(ckptPath (str)):
\begin{itemize}
\item transition: This exported access program first uses the Checkpoint Module's \ref{ModuleCkpt} loadCheckpoint access program with model and ckptPath as inputs to load the saved checkpoint into the model (changes the model's weights). This model is then run on the imageTens and lidarTens state variables to generate an output that is saved into the boundingBoxesPred state variable.
\item output: N/A
\item exception: N/A
\end{itemize}

\noindent inference(boundingBoxesGT (PyTorch Tensor $R^{g\times{}n_{b}}$)):
\begin{itemize}
\item transition: N/A 
\item output: This module uses the Plotting Module's \ref{ModulePlot} plot access program to display the lidar pointcloud alongside the predicted and ground truth bounding boxes. This is to allow for visual inspection of the predictions.
\item exception: N/A
\end{itemize}

\newpage

\section{MIS of Evaluation Module} \label{ModuleEval} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item PyTorch Dataloader (\href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{PyTorch Documentation})
  \item Model (\ref{ModuleModel})
  \item Checkpoint (\ref{ModuleCkpt})
\end{itemize}


\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
  \begin{tabular}{p{2cm}|p{6cm}|p{2cm}|p{2cm}}
  \hline
  \textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
  \hline
  eval & evalLoader (PyTorch Dataloader), model (Model Module \ref{ModuleModel}), ckpt (Checkpoint Module \ref{ModuleCkpt}) & metrics (Named Tuple of (str, $R$)) & - \\
  \hline
  \end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}


\subsubsection{Environment Variables}


\subsubsection{Assumptions}

\subsubsection{Access Routine Semantics}
\noindent eval(evalLoader (PyTorch Dataloader), model (Model Module), ckpt (Checkpoint Module)):
\begin{itemize}
\item transition: N/A 
\item output: This function will run the input model over the evaluation loader and collect the mAP statistics over this set of data. These statistics will then be returned as output. If a checkpoint is passed in, the model will first load the given checkpoint. Details of the mAP calculation can be seen in the SRS's IM4.
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}


\newpage

\section{MIS of Logger Module} \label{ModuleLog} 
This module is simply used to represent a wrapper around a global Tensorboard logger (\href{https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html}{PyTorch tutorial on using Tensorboard})
that can be used in any module to log metrics to a logging file. It interacts with the file system environment variable and can be used to visualize logs after saving them.

This module interacts with the File System environmental variable, saving log files to it.
\newpage

\section{MIS of Model Module} \label{ModuleModel} 

\subsection{Template}

\subsection{Uses}
\begin{itemize}
  \item Is a subclass of PyTorch Module (\href{https://pytorch.org/docs/stable/generated/torch.nn.Module.html}{PyTorch Documentation})
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
  \item Equivariant Layers (\ref{ModuleEQ})
  \item OpenPCDet Layers (\ref{ModulePCDet})
  \item Configuration (\ref{ModuleCfg})
\end{itemize}

\subsection{Syntax}

\subsubsection{Exported Constants}

\subsubsection{Exported Access Programs}

\begin{center}
  \begin{tabular}{p{2cm}|p{4cm}|p{4cm}|p{2cm}}
  \hline
  \textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
  \hline
  Model & - & Model & - \\
  \hline
  forward & imgTens (PyTorch Tensor $R^{n_{cams}\times{}3\times{}h\times{}w}$), lidarTens (PyTorch Tensor $R^{n_{l}\times{}3}$) & boundingBoxesPred (PyTorch Tensor $R^{g\times{}n_{\hat{b}}}$) & - \\
  \hline
  \end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}
\begin{itemize}
  \item pcdetLayers (OpenPCDet Layers Module \ref{ModulePCDet})
  \item eqLayers (Equivariant Layers Module \ref{ModuleEQ})
\end{itemize}

\subsubsection{Environment Variables}


\subsubsection{Assumptions}

\subsubsection{Access Routine Semantics}
\noindent Model():
\begin{itemize}
\item transition: This function will load the model-related configuration from the Config module \ref{ModuleCfg} and will then create the OpenPCDet and Equivariant Layers accordingly. References to these layers will be stored in the pcdetLayers and eqLayers state variables. The exact model setup can vary based on the model configuration input by the user (\href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{example}), but the idea here is to replace non-equivariant CNNs and transformers in the base BEVFusion model with SE(2)/SE(3) equivariant versions where possible (direct one-to-one swap/replacement). Certain layers such as RELU, flattening, backprojection ones are already equivariant or cannot be replaced and thus will be imported as is from OpenPCDet's existing layers. For a high-level overview of how the base BEVFusion architecture is structured, I would suggest to check out the BEVFusion paper (\cite{liang2022bevfusion}).
\item output: This function will return a reference to self.
\item exception: N/A
\end{itemize}

\noindent forward():
\begin{itemize}
\item transition: N/A 
\item output: This function will run the input data (LiDAR and Images) through the OpenPCDet and Equivariant layers to produce a set of predicted bounding boxes. These predicted bounding boxes will then be returned. 
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}


\newpage

\section{MIS of Loss Module} \label{ModuleLoss} 

\subsection{Template}



\subsection{Uses}

\begin{itemize}
  \item Is a subclass of PyTorch Loss (\href{https://pytorch.org/docs/stable/nn.html#loss-functions}{PyTorch Documentation})
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
  \item Configuration (\ref{ModuleCfg})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{4cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
Loss & - & Loss & - \\
\hline
forward & boundingBoxesPred (PyTorch Tensor $R^{g\times{}n_{\hat{b}}}$), boundingBoxesGT (PyTorch Tensor $R^{g\times{}n_{b}}$) & Loss Value (PyTorch Tensor $R$) & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

\begin{itemize}
  \item hyperparameters (Sequence of $R$)
\end{itemize}

\subsubsection{Environment Variables}

\subsubsection{Assumptions}

\subsubsection{Access Routine Semantics}
\noindent Loss():
\begin{itemize}
\item transition: This method will set the loss module's hyperparamters according to the loss-related configuration from the Config module \ref{ModuleCfg}. 
\item output: This method will output a reference to self.
\item exception: N/A
\end{itemize}

\noindent forward(boundingBoxesPred (PyTorch Tensor $R^{g\times{}n_{\hat{b}}}$), boundingBoxesGT (PyTorch Tensor $R^{g\times{}n_{b}}$)):
\begin{itemize}
\item transition: N/A 
\item output: This method will use the predicted and ground truth bounding boxes to regress and output a loss value. Details of the loss function to be implemented can be seen in the SRS's IM2.
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of Optimizer Module} \label{ModuleOptim} 

\subsection{Template}



\subsection{Uses}
\begin{itemize}
  \item Is a wrapper around the PyTorch ADAM Optimizer (\href{https://pytorch.org/docs/stable/optim.html}{PyTorch Documentation})
  \item PyTorch Parameter (\href{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html}{PyTorch Documentation})
  \item Configuration (\ref{ModuleCfg})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
Optimizer & modelParams (Sequence of PyTorch Parameter) & Optimizer & - \\
\hline
step & - & - & - \\
\hline
zero\_grad & - & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

\begin{itemize}
  \item hyperparameters (Sequence of $R$)
  \item modelParameters (Sequence of PyTorch Parameter)
\end{itemize}

\subsubsection{Environment Variables}



\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent Optimizer(modelParams (Sequence of PyTorch Parameter)):
\begin{itemize}
\item transition: This method will set the optimizer module's hyperparameters according to the optimizer-related configuration from the Config module \ref{ModuleCfg}. It will also save a reference to passed in model parameters in the modelParameters state variable. Details of the ADAM Optimizer can be seen in the SRS's IM3.
\item output: This method will return a reference to self.
\item exception: N/A
\end{itemize}

\noindent step():
\begin{itemize}
\item transition: This method will update the modelParameters based on their gradients, following the ADAM optimization method. Details of the ADAM Optimizer can be seen in the SRS's IM3.
\item output: N/A
\item exception: N/A
\end{itemize}

\noindent zero\_grad():
\begin{itemize}
\item transition: This method will clear all gradients for the modelParameters.
\item output: N/A
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of Plotting Module} \label{ModulePlot} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item Open3D Oriented Bounding Boxes (\href{https://www.open3d.org/docs/latest/python_api/open3d.geometry.OrientedBoundingBox.html}{Open3D Documentation})
  \item Open3D Pointclouds (\href{https://www.open3d.org/docs/release/python_api/open3d.geometry.PointCloud.html}{Open3D Documentation})
  \item Open3D in general (\href{https://www.open3d.org/docs/release/introduction.html}{Open3D Documentation})
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
plot & lidarTens (PyTorch Tensor $R^{n_{l}\times{}3}$), boundingBoxesGT (PyTorch Tensor $R^{g\times{}n_{b}}$), boundingBoxesPred (PyTorch Tensor $R^{g\times{}n_{\hat{b}}}$) & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

\subsubsection{Environment Variables}

\begin{itemize}
  \item Computer Screen (2D sequence of RGB tuples)
\end{itemize}

\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent plot():
\begin{itemize}
\item transition: The LiDAR pointcloud is first converted into an Open3D Pointcloud. Then, the bounding boxes are converted into the Open3d OrientedBoundingBox type. Finally, these are all visualized together in a single 3D plot using Open3D's draw\_geometries function. This outputs a plot and thus updates the computer screen environment variable.
\item output: N/A
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of Checkpoint Module} \label{ModuleCkpt} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item PyTorch Module (\href{https://pytorch.org/docs/stable/generated/torch.nn.Module.html}{PyTorch Documentation})
  \item PyTorch State Dictionary (\href{https://pytorch.org/tutorials/beginner/saving_loading_models.html}{PyTorch Documentation})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{3cm}|p{5cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
saveCheckpoint & model (PyTorch Module \ref{ModuleTorch}), CkptPath (str) & - & - \\
\hline
loadCheckpoint & model (PyTorch Module \ref{ModuleTorch}), CkptPath (str) & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}



\subsubsection{Environment Variables}

\begin{itemize}
  \item File System
\end{itemize}

\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent saveCheckpoint():
\begin{itemize}
\item transition: The input model's state dictionary is saved to a pytorch tensor file at the input path. 
\item output: N/A
\item exception: N/A
\end{itemize}


\noindent loadCheckpoint():
\begin{itemize}
\item transition: The input model's parameters are set using the state dictionary saved at the input path.
\item output: N/A
\item exception: N/A
\end{itemize}


\subsubsection{Local Functions}

\newpage

\section{MIS of Data Module} \label{ModuleData} 

\subsection{Module}



\subsection{Uses}
\begin{enumerate}
  \item PyTorch Dataset and Dataloader (\href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{PyTorch Documentation})
  \item Configuration (\ref{ModuleCfg})
\end{enumerate}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{3cm}|p{5cm}|p{4cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
Data & isEval (boolean) & Data & - \\
\hline
getDataloader & - & loader (PyTorch Dataloader) & -  \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}
\begin{itemize}
  \item dataset (PyTorch Dataset)
\end{itemize}

\subsubsection{Environment Variables}
\begin{enumerate}
  \item File System
\end{enumerate}

\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent Data():
\begin{itemize}
\item transition: This function will load the data-related configuration from the Config module \ref{ModuleCfg} and will then create the dataset accordingly.
This dataset will either be the NuScenes (\cite{caesar2020nuscenes}) or the Waymo (\cite{sun2020scalability}) dataset for now and will be stored in the dataset state variable. If isEval is false,
the training portion of the dataset will be loaded. If isEval is true, the evaluation portion of the dataset will be loaded.
\item output: This function will return a reference to self. 
\item exception: N/A
\end{itemize}

\noindent getDataloader():
\begin{itemize}
  \item transition: N/A
  \item output: This function will use the dataset state variable to create a PyTorch Dataloader which will then be returned.
  \item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of Equivariant Layers Module} \label{ModuleEQ} 

This module is meant to represent the equivariant layers I will be adding into the OpenPCDet repository for the novel part
of my project. To provide context for the reader, equivariant layers
refer to neural network layers that preserve input transformations when generating output features. For example, a rotated image of a cat 
will result in similarly rotated output features (when compared to features generated from an upright image of a cat). This work by Cohen et Al. (\cite{cohen2016group})
more clearly explains this concept.


This module will encapsulate two different types of equivariant layers. The first of these is steerable kernel 
equivariant CNNs (\cite{cohen2016steerable}) extended to include both 2D and 3D symmetries using the escnn repository (\cite{cesa2022a}). 
The second of these is equivariant transformer networks (\cite{tai2019equivariant}). Existing OpenPCDet layers can essentially then be directly
swapped out for these equivariant alternatives. 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item PyTorch Parameter (\href{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html}{PyTorch Documentation})
  \item Steerable CNNs (\cite{cesa2022a})
  \item Equivariant Transformers (\cite{tai2019equivariant})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{6cm}|p{4cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
init & - & - & - \\
\hline
forward & - & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}
\begin{itemize}
  \item layerParameters (Sequence of PyTorch Parameter)
\end{itemize}

\subsubsection{Environment Variables}



\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent init():
\begin{itemize}
\item transition: The parameters for the layer are randomly initialized according to a uniform distribution around 0 (saved in the layerParameters state variable). 
\item output: N/A
\item exception: N/A
\end{itemize}


\noindent forward():
\begin{itemize}
\item output:
\begin{itemize}
  \item \textbf{For the CNN case:} Since the CNN case is simpler, the math relating to it can be shown here itself. The result $y$ of this formula is returned:
  \newline\newline
  $y[m,n](k,x)=\sum_{i=-w}^{w}\sum_{j=-h}^{h}k[w+i,h+j]x[m+i,n+j]\rightarrow{}y(k,x)=k*x$ \newline
  such that if $x_{2}=gx_{1}$ for some $g$ in the group of considered transformations, $y(x_{2})=\rho(g)y(x_{1})$
  for a single function $\rho$.
  \newline
  \item \textbf{For the Transformer case:} The math here is more complicated and so I will simply refer the reader to the paper by Tai et. al (\cite{tai2019equivariant}). The output of this layer is returned. Generally though for attention function f,
  the following can be written similar to the CNN case:
  $x_{2}=gx_{1} \rightarrow{} y(k,x_{2})=\rho(g)y(k,x_{1})$ 
\end{itemize}
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of OpenPCDet Layers Module} \label{ModulePCDet} 
This module is simply used to represent the existing layers in the OpenPCDet library (\cite{openpcdet2020}).
Rather than highlight specific relevant layer types and modules, I would encourage the reader to read through the BEVFusion (\cite{liang2022bevfusion})
configuration file (\href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{BEVFusion Config File}) and 
associated code sections (\href{https://github.com/open-mmlab/OpenPCDet/tree/master/pcdet/models}{Model-Related OpenPCDet Code}).

\newpage

\section{MIS of PyTorch Module} \label{ModuleTorch} 
This module is simply used to represent the PyTorch library referenced in the other modules.
The main list of components that are used from this library is as follows:
\begin{itemize}
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
  \item PyTorch Module (\href{https://pytorch.org/docs/stable/generated/torch.nn.Module.html}{PyTorch Documentation})
  \item PyTorch Dataloader (\href{https://pytorch.org/tutorials/beginner/basics/data_tutorial.html}{PyTorch Documentation})
  \item PyTorch Optimizer (\href{https://pytorch.org/docs/stable/optim.html}{PyTorch Documentation})
  \item PyTorch Loss (\href{https://pytorch.org/docs/stable/nn.html#loss-functions}{PyTorch Documentation})
  \item PyTorch State Dictionary (\href{https://pytorch.org/tutorials/beginner/saving_loading_models.html}{PyTorch Documentation})
  \item PyTorch Parameter (\href{https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html}{PyTorch Documentation})
\end{itemize}

\newpage

\section{MIS of Config Module} \label{ModuleCfg} 

\subsection{Template}



\subsection{Uses}


\subsection{Syntax}



\subsubsection{Exported Constants}
\begin{itemize}
  \item CONFIG\_PATH (str): The standard filesystem path pointing to where the configuration file is. For now this is set to "./checkpoint" but the idea is to have it overridden through a command-line argument where necessary.  
\end{itemize}


\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{3cm}|p{5cm}|p{2cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
Config & - & Config & - \\
\hline
getModelConfig & - & - & - \\
\hline
getDataConfig & - & - & - \\
\hline
getOptimConfig & - & - & - \\
\hline
getLossConfig & - & - & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}

\begin{itemize}
  \item modelConfig (Sequence of $R$)
  \item dataConfig (Sequence of $R$)
  \item optimConfig (Sequence of $R$)
  \item lossConfig (Sequence of $R$)
\end{itemize}

\subsubsection{Environment Variables}

\begin{itemize}
  \item File System
\end{itemize}

\subsubsection{Assumptions}



\subsubsection{Access Routine Semantics}
\noindent Config():
\begin{itemize}
  \item transition: This method loads the yaml configuration file stored at the predefined constant path CONFIG\_PATH. Then, the dictionary loaded from this JSON file is split into model, data, optimizer, and loss sections. Each of these is saved to the corresponding state variable for this module. A sample configuration file can be seen \href{https://github.com/open-mmlab/OpenPCDet/blob/master/tools/cfgs/nuscenes_models/bevfusion.yaml}{here}.
  \item output: This method returns a reference to self.
  \item exception: N/A
\end{itemize}

\noindent getModelConfig():
\begin{itemize}
\item transition: N/A 
\item output: This method returns the configuration parameters for the model module \ref{ModuleModel}.
\item exception: N/A
\end{itemize}

\noindent getDataConfig():
\begin{itemize}
  \item transition: N/A
  \item output: This method returns the configuration parameters for the data module \ref{ModuleData}.
  \item exception: N/A
\end{itemize}

\noindent getOptimConfig():
\begin{itemize}
  \item transition: N/A
  \item output: This method returns the configuration parameters for the optimizer module \ref{ModuleOptim}.
  \item exception: N/A
\end{itemize}

\noindent getLossConfig():
\begin{itemize}
  \item transition: N/A
  \item output: This method returns the configuration parameters for the loss module \ref{ModuleLoss}.
  \item exception: N/A
\end{itemize}

\subsubsection{Local Functions}

\newpage

\section{MIS of Data Processing Module} \label{ModuleDataProc} 

\subsection{Module}



\subsection{Uses}
\begin{itemize}
  \item PyTorch Tensor (\href{https://pytorch.org/docs/stable/tensors.html}{PyTorch Documentation})
\end{itemize}

\subsection{Syntax}



\subsubsection{Exported Constants}



\subsubsection{Exported Access Programs}

\begin{center}
\begin{tabular}{p{2cm}|p{5cm}|p{3cm}|p{2cm}}
\hline
\textbf{Name} & \textbf{In} & \textbf{Out} & \textbf{Exceptions} \\
\hline
process & imgPath (str), lidarPath (str) & imgTens (PyTorch Tensor $R^{n_{cams}\times{}3\times{}h\times{}w}$), lidarTens (PyTorch Tensor $R^{n_{l}\times{}3}$) & - \\
\hline
\end{tabular}
\end{center}

\subsection{Semantics}

\subsubsection{State Variables}



\subsubsection{Environment Variables}



\subsubsection{Assumptions}

\newpage



\subsubsection{Access Routine Semantics}
\noindent process():
\begin{itemize}
\item transition: N/A 
\item output: Loads the multiview images and LiDAR pointcloud from their respective files and converts them into a standard PyTorch Tensor format that can be used by the model. The Tensor form of the multiview images and the LiDAR pointcloud are returned.
\item exception: N/A
\end{itemize}

\subsubsection{Local Functions}


\newpage

\bibliographystyle {plainnat}
\bibliography {../../../refs/References}

\newpage




\section{Appendix} \label{Appendix}

May be added in the future.


\end{document}